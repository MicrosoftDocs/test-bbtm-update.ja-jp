{
    "Slug": "advancing-microsoft-teams-on-azure-operating-at-pandemic-scale",
    "Title": "パンデミックスケールで運用される Azure でのMicrosoft Teamsの推進",
    "Summary": "Scale, resiliency, and performance do not happen overnight—it takes sustained and deliberate investment, day over day, and a performance-first mindset to build products that delight our users.",
    "Content": "<p>&ldquo;COVID-19パンデミックは、仕事、勉強、社会化の意味をリセットしました。 私たちの多くのように、私は同僚とのつながりとしてMicrosoft Teamsに頼るようになった。 この投稿では、Microsoft Teams製品グループ&mdash;<strong>のRish Tandon</strong> (コーポレート バイス プレジデント)、<strong>Aarthi Natarajan</strong> (グループ エンジニアリング マネージャー)、<strong>Martin Taillefer</strong> (アーキテクト)&mdash; の友人が、エンタープライズ レベルの安全な生産性アプリの管理とスケーリングに関する学習をいくつか共有しています。&rdquo;Mark Russinovich、CTO、Azure</p>\n\n<hr>\n<p>&nbsp;</p>\n\n<p>スケール、回復性、パフォーマンスは一晩&mdash;で起こりません。一日を通して、持続的かつ意図的な投資と、ユーザーを喜ばせる製品を構築するためのパフォーマンス優先の考え方が必要です。 発売以来、Teamsは、2017 年の発売から 2019 年 7 月の 1,300 万ユーザー、2019 年 11 月の 2,000 万ユーザーまで、強力な成長を経験しています。 4月には、Teamsは、1日にアクティブユーザーが7500万人以上、2億人の毎日の会議参加者、41億人の毎日の会議分があることを共有しました。 今までの急速な成長を受け、サービスを拡張するために必要な継続的な作業に慣れていると思Teams。 COVID-19 はこの仮定に挑戦しました。この経験は、以前は考えられない成長期の中でサービスを実行し続ける能力を与えてくれるでしょうか?</p>\n\n<h2>強固な基盤</h2>\n\n<p>Teamsはマイクロサービス アーキテクチャに基づいて構築されており、数百のマイクロサービスが連携して機能し、メッセージング、会議、ファイル、カレンダー、アプリなどの多くの機能を製品&rsquo;に提供します。 マイクロサービスを使用すると、各コンポーネント チームが個別に変更を処理およびリリースするのに役立ちます。</p>\n\n<p>Azure は、Microsoft Teamsを含むすべての Microsofts&rsquo; クラウド サービスを支えるクラウド プラットフォームです。 ワークロードは Azure 仮想マシン (VM) で実行され、以前のサービスは <a href=\"https://azure.microsoft.com/en-us/services/cloud-services/\" target=\"_blank\">Azure Cloud Services</a> 経由でデプロイされ、新しいサービスは <a href=\"https://azure.microsoft.com/en-us/services/service-fabric/\" target=\"_blank\">Azure Service Fabric</a>にデプロイされます。 プライマリ ストレージ スタックは <a href=\"https://azure.microsoft.com/en-us/services/cosmos-db/\" target=\"_blank\">Azure Cosmos DB</a> であり、一部のサービスでは <a href=\"https://azure.microsoft.com/en-us/services/storage/blobs/\" target=\"_blank\">Azure Blob Storage</a> を使用しています。 スループットと回復性を高めるには<a href=\"https://azure.microsoft.com/en-us/services/cache/\" target=\"_blank\">、Azure Cache for Redis</a>が必要です。 <a href=\"https://azure.microsoft.com/en-us/services/traffic-manager/\" target=\"_blank\">Traffic Manager</a>と <a href=\"https://azure.microsoft.com/en-us/services/frontdoor/\" target=\"_blank\">Azure Front Door</a> を利用して、必要な場所にトラフィックをルーティングします。 <a href=\"https://azure.microsoft.com/en-us/services/storage/queues/\" target=\"_blank\">キュー Storage</a>と <a href=\"https://azure.microsoft.com/en-us/services/event-hubs/\" target=\"_blank\">Event Hubs</a> を使用して通信し、テナントとユーザーを管理する<a href=\"https://azure.microsoft.com/en-us/services/active-directory/\" target=\"_blank\">Azure Active Directory</a>に依存しています。</p>\n\n<p>&nbsp;</p>\n\n<p><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/18c7ac34-1b68-4073-b549-945fdda49e0e.png\"><img alt=\"    Diagram showing that Azure is the platform that underpins Teams Services and Office 365 Core Service\" border=\"0\" height=\"426\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/c229cba7-597c-46dc-8545-7f42706d8c55.png\" style=\"border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\" title=\"\" width=\"640\"></a></p>\n\n<p>&nbsp;</p>\n\n<p>この投稿は主にクラウド バックエンドに焦点を当てていますが、&rsquo;Teams クライアント アプリケーションでも最新の設計パターンとフレームワークを使用し、豊富なユーザー エクスペリエンスを提供し、オフラインまたは断続的に接続されたエクスペリエンスをサポートしていることを強調する価値があります。 クライアントを迅速かつサービスと連携して更新するコア機能は、迅速な反復のための重要なイネーブラーです。 &rsquo;アーキテクチャの詳細については、<a href=\"https://myignite.techcommunity.microsoft.com/sessions/83471\" target=\"_blank\">Microsoft Ignite 2019</a> のこのセッションを参照してください。</p>\n\n<h2>アジャイル開発</h2>\n\n<p>CI/CD パイプラインは、<a href=\"https://docs.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops\" target=\"_blank\">Azure Pipelines</a>の上に構築されています。 自動化されたエンドツーエンドテストとテレメトリ信号の組み合わせに基づくゲートを使用して、リングベースのデプロイ戦略を使用します。 テレメトリ信号は、インシデント管理パイプラインと統合され、サービスとクライアント定義の両方のメトリックに対するアラートを提供します。 分析には <a href=\"https://azure.microsoft.com/en-us/services/data-explorer/\" target=\"_blank\">Azure Data Explorer</a> に大きく依存しています。</p>\n\n<p>さらに、クラッシュ率、メモリ消費量、アプリケーションの応答性、パフォーマンス、ユーザー エンゲージメントなどの主要な製品メトリックに対して特徴の動作を評価するスコアカードを含む実験パイプラインを使用します。 これにより、新機能が望むように機能しているかどうかを把握できます。</p>\n\n<p>すべてのサービスとクライアントは、一元化された構成管理サービスを使用します。 このサービスは、製品機能のオンとオフの切り替え、キャッシュの有効期間の値の調整、ネットワーク要求の頻度の制御、API に対して接続するネットワーク エンドポイントの設定を行う構成状態を提供します。 これにより、暗く起動し &ldquo;、A/B テストを実行して、&rdquo; 変更の影響を正確に測定して、すべてのユーザーが安全かつ効率的であることを確認するための柔軟なフレームワークが提供されます。</p>\n\n<h2>主な回復性戦略</h2>\n\n<p>私たちは、サービスの艦隊全体でいくつかの回復性戦略を採用しています。</p>\n\n<ul>\n    <li><strong>アクティブ/アクティブフォールト トレラント システム</strong>: アクティブ/アクティブフォールト トレラント システムは、運用上独立した 2 つ以上の異種パスとして定義されます。各パスは、安定した状態でライブ トラフィックを提供するだけでなく、クライアントとプロトコルのパス選択を利用してシームレス フェールオーバーを行いながら、予想されるトラフィックの 100% を処理する機能も備えています。 この戦略は、異種システムの構築と維持を正当化するために合理的なコストで非常に大きな障害ドメインまたは顧客の影響がある場合に採用します。 たとえば、外部に表示されるすべてのクライアント ドメインに対してOffice 365 DNS システムを使用します。 さらに、静的CDN クラスのデータは、Azure Front Door と Akamai の両方でホストされます。</li>\n    <li><strong>回復性に最適化されたキャッシュ</strong>: パフォーマンスと回復性の両方のために、コンポーネント間のキャッシュを広範囲に活用します。 キャッシュは、ダウンストリーム サービスが利用できない場合に備えて、平均待機時間を短縮し、データソースを提供するのに役立ちます。 データを長期間キャッシュに保持すると、データの最新性の問題が発生しますが、データを長期間キャッシュに保持することは、ダウンストリームの障害に対する最善の防御策です。 キャッシュ データへの Time to Refresh (TTR) と Time to Live (TTL) に重点を置いています。 長い TTL と短い TTR 値を設定することで、データを最新の状態に保つ方法と、ダウンストリームの依存関係が失敗するたびにデータを保持する期間を微調整できます。</li>\n    <li><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker\" target=\"_blank\"><strong>サーキット ブレーカー</strong></a>: これは、サービスが失敗する可能性が高い操作を実行できないようにする一般的な設計パターンです。 これは、ダウンストリーム サービスが再試行要求に圧倒されることなく復旧する機会を提供します。 また、依存関係に問題がある場合のサービスの応答も向上し、システムのエラー状態に対する耐性を高めます。</li>\n    <li><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead\" target=\"_blank\"><strong>一括分離</strong></a>: 重要なサービスの一部を完全に分離されたデプロイに分割します。 1 つのデプロイで問題が発生した場合、一括分離は、他のデプロイが動作を継続するのに役立つよう設計されています。 この軽減策により、可能な限り多くのお客様の機能が維持されます。</li>\n    <li><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/throttling\" target=\"_blank\"><strong>API レベルのレート制限</strong></a>: 重要なサービスが API レベルで要求を調整できるようにします。 これらのレート制限は、上記で説明した一元化された構成管理システムを通じて管理されます。 この機能により、COVID-19 の急増時にクリティカルでない API をレート制限することが可能になりました。</li>\n    <li><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/retry\" target=\"_blank\"><strong>効率的な再試行パターン</strong></a>: すべての API クライアントが効率的な再試行ロジックを実装していることを確認し、検証します。これにより、ネットワーク障害が発生したときにトラフィック ストームが発生しないようにします。</li>\n    <li><strong>タイムアウト</strong>: タイムアウト セマンティクスを一貫して使用すると、ダウンストリームの依存関係で何らかの問題が発生しているときに作業が停止するのを防ぐことができます。</li>\n    <li><strong>ネットワーク障害の適切な処理</strong>: オフライン時や接続が不十分な場合に、クライアント エクスペリエンスを向上させるために長期的な投資を行いました。 COVID-19の急増が始まったのと同じように、この分野の大幅な改善が開始され、ネットワーク品質に関係なく一貫した経験を提供することができます。</li>\n</ul>\n\n<p>Azure <a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/\" target=\"_blank\">Cloud Design Patterns</a> を確認したことがある場合は、これらの概念の多くがよく理解できる場合があります。&nbsp; また、これらのパターンの一部の実装を提供するマイクロサービスでは、 <a href=\"https://github.com/App-vNext/Polly\" target=\"_blank\">Polly ライブラリ</a> を幅広く使用しています。</p>\n\n<p>私たちのアーキテクチャはうまく機能していましたが、Teams使用は月を超えて増加しており、プラットフォームは需要に合わせて簡単にスケーリングされました。 ただし、スケーラビリティは一連&rdquo;の&ldquo;考慮事項ではなく、複雑なシステムで現れる新しい動作に対処するために継続的な注意が必要です。</p>\n\n<p><strong>COVID-19の自宅での注文が世界中で始まったとき、急速に増加する需要に効果的に対応するために、システムに組み込まれているアーキテクチャの柔軟性を活用し、可能なすべてのノブを回す必要があります。</strong></p>\n\n<h2>容量予測</h2>\n\n<p>他の製品と同様に、生のユーザーと使用パターンの両方で、成長が発生する場所を予測するために、モデルを構築し、常に反復処理します。 モデルは、履歴データ、循環パターン、新しい大規模顧客、およびその他のさまざまなシグナルに基づいています。</p>\n\n<p>急増が始まるにつれて、以前の予測モデルはすぐに時代遅れになっていることが明らかになったので、世界的な需要の大きな成長を考慮に入れた新しいモデルを構築する必要がありました。 既存のユーザーからの新しい使用パターン、既存のユーザーと休止中のユーザーからの新しい使用状況、および製品にオンボードする多くの新しいユーザーが同時に見られました。 さらに、潜在的なコンピューティングとネットワークのボトルネックに対処するために、高速化されたリソースの決定を行う必要がありました。 複数の予測モデリング手法 (<a href=\"https://otexts.com/fpp2/arima.html\" target=\"_blank\">ARIMA</a>、Additive、Multiplicative、対数) を使用します。 そのため、過剰予測を回避するために、国ごとの基本的な上限を追加しました。 業界や地域ごとの使用による変曲と成長パターンを理解しようと、モデルをチューニングしました。 COVID-19 影響日に関 <a href=\"https://coronavirus.jhu.edu/\" target=\"_blank\">する Johns Hopkins&rsquo;</a> の調査を含む外部データ ソースを各国別に組み込み、ボトルネック地域のピーク負荷予測を強化しました。</p>\n\n<p>プロセス全体を通して、使用パターンが安定するにつれて、慎重に誤りを犯し、過剰プロビジョニングを&mdash;優先しました。また、必要に応じてスケールバックしました。</p>\n\n<h2>コンピューティング リソースのスケーリング</h2>\n\n<p>一般に、自然災害に耐えるTeamsを設計しています。 複数の Azure リージョンを使用すると、データセンターの問題だけでなく、主要な地理的領域への中断からリスクを軽減するのに役立ちます。 ただし、これは、このような結果の間に影響を受けるリージョン&rsquo;の負荷に対応できるように、追加のリソースをプロビジョニングすることを意味します。 スケールアウトするために、すべての重要なマイクロサービスのデプロイを、すべての主要な Azure 地域の追加リージョンに迅速に拡張しました。 地域ごとのリージョンの合計数を増やすことで、緊急負荷を吸収するために保持する必要がある各リージョンのスペア容量の合計量を減らし、総容量のニーズを減らしました。 この新しいスケールで負荷を処理すると、効率を向上させる方法についていくつかの分析情報が得られます。</p>\n\n<ul>\n    <li>マイクロサービスの一部を再デプロイして、より小さなコンピューティング クラスターの数を増やすことで、クラスターごとのスケーリングに関するいくつかの考慮事項を回避し、デプロイの高速化に役立ち、よりきめ細かな負荷分散を実現できました。</li>\n    <li>以前は、さまざまなマイクロサービスに使用する特定の仮想マシン (VM) の種類に依存しました。 VM の種類や CPU の点で柔軟性が高く、全体的なコンピューティング能力やメモリに重点を置くことで、各リージョンで Azure リソースをより効率的に使用することができました。</li>\n    <li>サービス コード自体に最適化の機会が見つかりました。 たとえば、いくつかの簡単な改善により、アバターの生成に費やす CPU 時間が大幅に削減されました (ユーザーの画像がないときに使用されるイニシャルを含む小さなバブル)。</li>\n</ul>\n\n<h2>ネットワークとルーティングの最適化</h2>\n\n<p>Teams&rsquo;容量の消費量のほとんどは、特定の Azure 地域で日中に発生し、夜間にリソースがアイドル状態になります。 このアイドル状態の容量を活用するためのルーティング戦略を実装しました (常にコンプライアンスとデータ所在地の要件を尊重します)。</p>\n\n<ul>\n    <li>非対話型バックグラウンド作業は、現在アイドル状態の容量に動的に移行されます。 これは、トラフィックが適切な場所に確実に到達するように、Azure Front Door で API 固有のルートをプログラミングすることによって行われます。</li>\n    <li>通話と会議のトラフィックは、急増に対処するために複数のリージョンにルーティングされました。 Azure Traffic Managerを使用して負荷を効果的に分散し、観察された使用パターンを活用しました。 また、ワイド エリア ネットワーク (WAN) の調整を防ぐために、時間帯の負荷分散を行う Runbook の作成にも取り組んだ。</li>\n</ul>\n\n<p>一部のTeams&rsquo; クライアント トラフィックは、Azure Front Door で終了します。 ただし、より多くのリージョンにクラスターをデプロイするにつれて、新しいクラスターで十分なトラフィックが得られないことがわかりました。 これは、ユーザーの場所と Azure Front Door ノードの場所の分布の成果物でした。 このトラフィックの偏在に対処するために、Azure Front Doors&rsquo; 機能を使用して国レベルでトラフィックをルーティングしました。 この例では、1 つのサービスに対して追加のフランスのトラフィックを英国西部リージョンにルーティングした後、トラフィックの分散が改善されたことを次に示します。</p>\n\n<p align=\"center\"><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/8125fe92-6b30-4d30-846c-fb22fa82d630.png\"><img alt=\" Graph showing improved traffic distribution after routing additional France traffic to the UK West region\" border=\"0\" height=\"284\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/588d8f8c-4b7d-4e63-8af5-2989021fbcab.png\" style=\"border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\" title=\"\" width=\"1024\"></a><em>&nbsp;<br>\n図 1: リージョン間でトラフィックをルーティングした後のトラフィック分散が改善されました。</em></p>\n\n<h2>キャッシュとストレージの機能強化</h2>\n\n<p>多くの分散キャッシュを使用しています。 大きな分散キャッシュの多く。 トラフィックが増加するにつれて、個々のキャッシュがスケーリングされないポイントまでキャッシュの負荷も増加しました。 キャッシュの使用に大きな影響を与えるいくつかの簡単な変更をデプロイしました。</p>\n\n<ul>\n    <li>生の JSON ではなくバイナリ形式でキャッシュ状態を格納し始めました。 これにプロトコル バッファー形式を使用しました。</li>\n    <li>キャッシュに送信する前にデータの圧縮を開始しました。 圧縮比に対して優れた速度のため、LZ4 圧縮を使用しました。</li>\n</ul>\n\n<p>ペイロード サイズを 65% 削減し、逆シリアル化時間を 40% 削減し、シリアル化時間を 20% 削減することができました。 すべての周りの勝利。</p>\n\n<p>調査の結果、いくつかのキャッシュが過度に積極的な TTL 設定を持ち、不要な熱心なデータの削除が発生したことが明らかになります。 これらの TTL を増やすと、ダウンストリーム システムでの平均待機時間と負荷の両方が削減されました。</p>\n\n<h2>目的に合った劣化 (機能のブラウンアウト)</h2>\n\n<p>どの&rsquo;程度まで&rsquo;プッシュする必要があるのかわからないので、追加のTeams容量をオンラインにするための時間を購入するために、予期しない需要の急増に迅速に対応できるメカニズムを配置することが賢明であると判断しました。</p>\n\n<p>すべての機能が、お客様にとって同等の重要性を持つわけではありません。 たとえば、メッセージの送受信は、他のユーザーが現在メッセージを入力していることを確認する機能よりも重要です。 このため、サービスのスケールアップに取り組んでいる間、入力インジケーターを 2 週間オフにしました。 これにより、インフラストラクチャの一部に対してピーク トラフィックが 30% 減少しました。</p>\n\n<p>通常、アーキテクチャの多くのレイヤーでアグレッシブなプリフェッチを使用するため、必要なデータが手元に近づき、エンド ツー エンドの平均待機時間が短縮されます。 ただし、プリフェッチは、使用されないデータをフェッチする際に多少の無駄な作業が発生し、プリフェッチされたデータを保持するためにストレージ リソースが必要であるため、コストが高くなる可能性があります。 一部のシナリオでは、プリフェッチを無効にし、一部のサービスで待機時間が長くなるコストで容量を解放することを選択しました。 それ以外の場合は、プリフェッチ同期間隔の期間を長くしました。 このような例の 1 つは、要求ボリュームを 80% 削減したモバイルでの予定表プリフェッチの抑制です。<br>\n&nbsp;</p>\n\n<p align=\"center\"><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/6529abae-ee4b-469a-80f8-cb8d7c8c224d.png\"><img alt=\" Graph showing that suppressing calendar prefetch on mobile reduced request volume by 80%\" border=\"0\" height=\"589\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/4f02c6cf-a077-4d97-9620-1b410bbfdd1e.png\" style=\"border: 0px currentcolor; border-image: none; display: inline; background-image: none;\" title=\"\" width=\"970\"></a><br>\n<em>図 2: モバイルで予定表イベントの詳細のプリフェッチを無効にします。</em></p>\n\n<h2>インシデント管理</h2>\n\n<p>システムの正常性を追跡および維持するために使用する成熟したインシデント管理プロセスがある一方で、このエクスペリエンスは異なっていました。 私たちはトラフィックの急増に対処するだけでなく、エンジニアや同僚は、自宅で働くことに適応しながら、個人的で感情的な課題を経験していました。</p>\n\n<p>お客様だけでなくエンジニアのサポートも確実に行うために、いくつかの変更を加えました。</p>\n\n<ul>\n    <li>インシデント管理のローテーションを週単位から毎日の周期に切り替えました。</li>\n    <li>すべてのオンコール エンジニアは、シフトの間に少なくとも 12 時間の休暇を取っていました。</li>\n    <li>組織全体からより多くのインシデント マネージャーを導入しました。</li>\n    <li>サービス全体で重要でない変更をすべて延期しました。</li>\n</ul>\n\n<p>これらの変更により、すべてのインシデント マネージャーとオンコール エンジニアが、お客様の要求を満たしながら、自宅のニーズに集中するのに十分な時間が確保されました。</p>\n\n<h2>Teamsの未来</h2>\n\n<p>数年前に起こった場合、この状況がどうなっていただろうかと振り返って不思議に思うのは魅力的です。 クラウド コンピューティングを使用せずに行ったようにスケーリングすることは不可能でした。 構成ファイルを変更するだけで今日できることは、以前は新しい機器や新しい建物を購入する必要がありました。 現在のスケーリングの状況が安定するにつれて、我々は将来に注意を戻しています。 インフラストラクチャを改善する多くの機会があると考えています。</p>\n\n<ul>\n    <li>Azure Kubernetes Serviceを使用して VM ベースのデプロイからコンテナー ベースのデプロイに移行する予定です。これにより、運用コストの削減、機敏性の向上、業界との連携が期待されます。</li>\n    <li>REST の使用を最小限に抑え、gRPC などのより効率的なバイナリ プロトコルを優先することを期待しています。 システム全体のポーリングの複数のインスタンスを、より効率的なイベント ベースのモデルに置き換えます。</li>\n    <li>私たちは、システムを信頼性の高いものにするために配置されたすべてのメカニズムが常に完全に機能し、行動に移る準備ができていることを保証するために、カオスエンジニアリングプラクティスを体系的に受け入れています。</li>\n</ul>\n\n<p>アーキテクチャを業界のアプローチに合わせて維持し、Azure チームのベスト プラクティスを活用することで、専門家はデータ分析、監視、パフォーマンスの最適化、インシデント管理などの問題を迅速に解決できます。 Microsoft およびより広範なソフトウェア開発コミュニティの同僚の開放性に感謝しています。 アーキテクチャとテクノロジは重要ですが、システムを正常に保つのは、ユーザーのチームです。</p>\n\n<p>&nbsp;</p>\n\n<hr>\n<p><em>関連する投稿: </em><a href=\"https://aka.ms/AdvancingReliability/6\" target=\"_blank\"><em>Azure は COVID-19 に応答します</em></a><em>。<br>\n関連記事: </em><a href=\"https://aka.ms/AA8ad9b\" target=\"_blank\"><em>COVID-19 パンデミック中に、お客様の Microsoft を支援するために Azures&rsquo; 容量を増やします</em></a><em>。</em></p>\n"
}