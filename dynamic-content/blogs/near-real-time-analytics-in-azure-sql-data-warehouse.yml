### YamlMime:Yaml
ms.openlocfilehash: c075507fe9b08fd6125482ed62f763711d07d47b
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/11/2022
ms.locfileid: "139892870"
Slug: near-real-time-analytics-in-azure-sql-data-warehouse
Title: Azure SQL Data Warehouse でのほぼリアルタイムの分析
Summary: IoT シナリオとデータ ストリームを通じてデータ作成のペースが増加するに応じて、分析のためにデータに高速にアクセスする需要が高まっています。
Content: >-
  <p>IoT シナリオとデータ ストリームを通じてデータ作成のペースが増加するに応じて、分析のためにデータに高速にアクセスする需要が高まっています。 このほぼリアルタイムの分析の需要は、小売の大手企業がリアルタイムで価格変更を行い、異常検出を使用して工場を製造する工場まで、異常検出を使用してアセンブリ ライン上の潜在的な問題を特定し、&#39;が発生している地球の数百フィート下で何が起こっているかを正確に知る、技術の高いドリル センサーの測定値を使用する会社に対して見られます。&#39; ほぼリアルタイムの分析の利点は非常に大きい可能性があるというすべてのお客様の間で確認しました。</p>


  <p>本日は、Azure SQL Data Warehouse でほぼリアルタイムの分析機能をお知<a href="https://azure.microsoft.com/en-us/services/sql-data-warehouse/" target="_blank">らせします</a>。 このアーキテクチャは、ストリーミング データフレームから SQL DW へのストリーミング インジェストAzure Databricksによって可能になります。</p>


  <h2>Azure Databricks での構造化ストリーミング</h2>


  <p>Azure Databricksは、Microsoft が提供するフル マネージドクラウド サービスで、Databricks Runtime。 このサービスは、Azure でのエンタープライズ レベルのApache Spark実装を提供します。 <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#overview" target="_blank">Apache Spark</a> の構造化ストリーミングを使用すると、ユーザーはスケーラブルでフォールト トレラントな方法でデータ ストリームに対するクエリを定義できます。</p>


  <p>構造化ストリーミングは、データ ストリームに対してクエリを実行するためのスケーラブルでフォールト トレラントな方法です。 ストリーミング データフレームは非インバウンド テーブルであり、ストリームからの新しいデータがテーブルに追加されます。 クエリは、追加されたセクションに対して実行し、テーブル全体にわたって実行できます。</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/0647a5fe-034a-4729-87cd-31019e84c648.jpg"><img alt="Data stream" border="0" height="874" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/be25da76-35cb-477a-b996-75ca8bed5854.jpg" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="データ ストリーム" width="1821"></a></p>


  <h2>Azure SQL Data Warehouseシンクとして使用</h2>


  <p>ストリーミング クエリは、時間の平均、最小値、最大値など、データのストリームに対する明らかな質問に答えるのに最適ですが、ダウンストリーム アナリストがほぼリアルタイムのデータにアクセスできるとは言えます。 これを実現するには、可能な限り迅速に SQL Data Warehouse にデータを取得し、アナリストが PowerBI などのツールを使用して、ほぼリアルタイムのデータのクエリ、視覚化、解釈を行う必要があります。</p>


  <h2>例: DW にストリームをSQL例</h2>


  <p>この機能のしくみを説明する簡単な例です。 Structured Stream には、1 秒あたりの特定のレートでタイムスタンプと値を生成するデータ ジェネレーターがあります。 このメカニズムを使用して、単純なストリーミングの例を作成します。</p>


  <p>最初に行う必要があるのは、シンク テーブルをテーブルに作成SQL Data Warehouse。 前述のように、レート ストリームではタイムスタンプと値が生成されます。そのため、それをテーブルのスキーマとして使用できます。 これを行うには、お気に入りのデータベース管理ツールを開き、次の表を作成します。</p>


  <pre>

  ```sql

  CREATE TABLE [dbo].[Stream_CI]

  (
     [timestamp] DATETIME NULL,
     [Value] BIGINT NULL
  )

  WITH

  (
     DISTRIBUTION = ROUND_ROBIN,
     CLUSTERED INDEX ([timestamp])
  )

  ```

  A table with a ROUND_ROBIN distribution and a CLUSTERED INDEX on Timestamp provides the best compromise between ingestion speed and query performance for streaming data in a SQL Data Warehouse. </pre>


  <p>DW にシンク テーブルが作成SQL、&#39;部分をAzure Databricksできます。 以下のコードはすべて Python で記述しましたが、R、Python、Scala でも同じ機能を使用できます。</p>


  <p>最初に行う必要があるのは、SQL DW への接続を設定し、ここでテーブルを作成し、Azure Storageします。</p>


  <pre>

  ```python

  # SQL DW related settings (used to setup the connection to the SQL DW instance)

  dwDatabase = &lt;databaseName&gt;

  dwServer = &lt;servername&gt;

  dwUser = &lt;sqlUser&gt;

  dwPass = &lt;sqlUserPassword&gt;

  dwJdbcPort =  &quot;1433&quot;

  dwJdbcExtraOptions = &quot;encrypt=true;trustServerCertificate=true;loginTimeout=30;&quot;

  sqlDwUrl = &quot;jdbc:sqlserver://&quot; + dwServer + &quot;.database.windows.net:&quot; + dwJdbcPort + &quot;;database=&quot; + dwDatabase + &quot;;user=&quot; + dwUser+&quot;;password=&quot; + dwPass + &quot;;&quot;+dwJdbcExtraOptions

  # Blob Storage related settings (used for temporary storage)

  # The value is the storage account url in the format of &lt;accountName&gt;.blob.core.windows.net

  blobStorage = &lt;blobStorageAccount&gt;

  blobContainer = &lt;storageContainer&gt;

  blobAccessKey =  &lt;accessKey&gt;

  # Set up the Blob storage account access key in the notebook session conf.

  spark.conf.set(
     &quot;fs.azure.account.key.&quot;+blobStorage ,
     blobAccessKey)
  ```</pre>


  <p>このコード ブロックでは、ユーザー名とパスワードを使用して、配信の簡略化と速度を実現していますが、これはベスト プラクティスではありません。 シークレット スコープを <a href="https://docs.azuredatabricks.net/user-guide/secrets/secret-scopes.html" target="_blank">使用して、シークレット</a> をセキュリティで保護された場所に格納Azure Key Vault。</p>


  <p>SQL DW と Storage アカウントの両方に Azure Databricks から接続できるので、&#39;は読み取りストリームを作成し、出力を SQL DW に書き込みます。</p>


  <p>レート ストリームの最も重要なパラメーターは、rowsPerSecond と numPartitions です。 rowsPerSecond は、システムが作成を試みる 1 秒あたりのイベント数を指定します。 numPartitions は、行の作成に割り当てられるパーティションの数を指定します。 rowsPerSecond が高く、システムが十分なデータを生成していない場合は、より多くのパーティションを使用してみてください。</p>


  <pre>

  ```python

  # Prepare streaming source

  df = spark.readStream \
     .format(&quot;rate&quot;) \
     .option(&quot;rowsPerSecond&quot;, &quot;10000&quot;) \
     .option(&quot;numPartitions&quot;, &quot;5&quot;) \
     .load()
  ```</pre>


  <p>DW に readStream をSQLするには&quot;、com.databricks.spark.sqldw 形式を使用する必要&quot;があります。 このフォーマット オプションは DataBricks ランタイムに組み込まれており、Databricks 4.3 以上を実行しているすべてのクラスターで使用できます。</p>


  <pre>

  ```python

  # Structured Streaming API to continuously write the data to a table in SQL DW.

  df.writeStream \
     .format(&quot;com.databricks.spark.sqldw&quot;) \
     .option(&quot;url&quot;, sqlDwUrl) \
     .option(&quot;tempDir&quot;, &quot;wasbs://&quot;+blobContainer+ &quot;@&quot; + blobStorage + &quot;/tmpdir/stream&quot;) \
     .option(&quot;forwardSparkAzureStorageCredentials&quot;, &quot;true&quot;) \
     .option(&quot;dbTable&quot;, &quot;Stream_CI&quot;) \
     .option(&quot;checkpointLocation&quot;, &quot;/checkpoint&quot;) \
     .trigger(processingTime=&quot;30 seconds&quot;) \
     .start()
  ```</pre>


  <p>このステートメントは、rateStream を 30 秒ごとに処理します。 データはストリームから取得され、tempDir パラメーターで定義された一時ストレージの場所に書き込まれます。 ファイルがこの場所に保存された後、SQL DW は PolyBase を使用して指定されたテーブルにデータを読み込む必要があります。 読み込みが完了すると、ストレージの場所のデータが削除され、データが 1 回だけ読み取りされます。<br>

  ストリームがデータを生成し、それを SQL DW に書き込むので、DW 内のデータに対してクエリを実行SQLできます。</p>


  <pre>

  ```sql

  SELECT COUNT(Value), DATEPART(mi,[timestamp]) AS [event_minute]

  FROM Stream_ci

  GROUP BY DATEPART(mi,[timestamp])

  ORDER BY 2

  ```</pre>


  <p>同様に、DW でストリーミング データのクエリをSQLできます。</p>


  <h2>まとめ</h2>


  <p>このブログでは、&#39;構造化ストリーミング機能と Azure Databricks&#39; SQL Data Warehouse を使用してほぼリアルタイムの分析を有効にする方法の簡単な例について説明しました。 この例では、わかりやすくするために rateStream と単純な書き込みコマンドを使用しましたが、Kafka ストリームをデータ ソースとして簡単に使用し、ビジネス固有のタンブリング ウィンドウ集計を使用できます。</p>


  <p>詳細については、以下を参照してください。</p>


  <ul>
   <li>Azure SQL <a href="https://azure.microsoft.com/en-us/blog/redefine-data-analytics-with-modern-data-warehouse-on-azure/" target="_blank">DW が最新のサービス更新プログラムを使用してデータ分析を再定義する方法について説明します</a></li>
   <li>ユーザーが<a href="https://azure.microsoft.com/en-us/blog/seamless-access-to-management-insights-for-sql-data-warehouse-with-data-studio/" target="_blank">管理するメンテナンスウィンドウによって、AZURE と DW のお客様のSQLする方法を確認します</a></li>
   <li>Azure SQL DW 統合と SQL Studio を使用して顧客がより優<a href="https://azure.microsoft.com/en-us/blog/azure-sql-data-warehouse-now-supports-maintenance-scheduling/" target="_blank">れた分析情報を得る方法を確認する</a></li>
  </ul>
