### YamlMime:Yaml
ms.openlocfilehash: 78876e9614f9df791d300c4ebe358014d9c83f79
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/11/2022
ms.locfileid: "139904258"
Slug: bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus
Title: Bing は、Azure gpu を使用した検索エクスペリエンスの最大の向上を実現します
Summary: 最近数年で、ディープラーニングが Bing 検索スタック全体に広く採用され、非常に多くのインテリジェントな機能を備えています。
Content: >-
  <p>最近数年で、ディープラーニングは Bing 検索スタック全体に広く採用されており、非常に多くのインテリジェントな機能を備えています。 自然言語モデルを使用して、ユーザー &rsquo; の検索インテントと関連する web ページに対する主要な検索アルゴリズム &rsquo; の理解を向上させ、Bing が最も関連性の高い検索結果をユーザーに提供できるようにします。 ここでは、ディープラーニングコンピューターのビジョン手法を使用して、テキスト記述や概要メタデータが付属していない場合 &rsquo; でも、数十億のイメージの検出可能性を高めています。 コンピューターベースの読みやすさモデルを活用して、ユーザーの特定の質問に直接回答する大きなテキスト本文内のキャプションを取得します。 これらのすべての拡張機能は、web 検索クエリの関連するコンテキストの結果をより詳細に導き出します。</p>


  <p>最近では、"トランスフォーマー" と呼ばれるモデルの種類 (おり世、 <a href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Farxiv.org%2Fabs%2F1810.04805&amp;data=02%7C01%7Cv-carjen%40microsoft.com%7C60eee5593b424638a57908d76bc14484%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637096350133971160&amp;sdata=5g6BsXkAcxjMTsG%2FbxTgUN4hqLbl2nmfcE9z%2B5LaM%2F8%3D&amp;reserved=0">BERT</a>) によって自然言語の理解が画期的になりました。 単語を順番に処理する前のディープニューラルネットワーク (DNN) アーキテクチャとは異なり、トランスフォーマーは、各単語とその周囲にあるすべての単語のコンテキストおよび関係を文で認識します。 今年の4月から、大規模なトランスフォーマーモデルを使用して、過去1年の Bing のお客様に最大の品質向上を実現しました。 たとえば、クエリ &quot; で aggravate a concussion &quot; を使用した場合、aggravate &quot; は、concussion の後に実行するアクションについて学習し、原因や症状を知りたいという言葉 &quot; を示しています。 これらのモデルを活用した検索では、ユーザーの意図を理解し、より有用な結果を提供できるようになりました。 さらに重要な点として、これらのモデルはすべての Bing 検索クエリに適用され、Bing の結果がより関連性が高く、インテリジェントになりました。</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/f32c5d70-d3c7-4c1d-ad12-35fa5f1fdb4e.jpg"><img alt="rankBERTblogbeforeafter" border="0" height="292" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/4ae2fecf-0ff2-4765-98ae-7172103207c4.jpg" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="rankBERTblogbeforeafter" width="1024"></a></p>


  <h2>Web 検索スケールのディープラーニングが非常に高価になる</h2>


  <p>Bing のお客様は、非常に高速な検索エクスペリエンスを期待し、ミリ秒ごとに待機時間が重要になります。 &nbsp;トランスフォーマーベースのモデルは、最大で数十億のパラメーターを使用して事前トレーニングされています。これは、以前のネットワークアーキテクチャと比較して、パラメーターのサイズと計算要件が大きくなります。 20個の CPU コアの待機時間を提供する distilled の3層 BERT モデルは、推論ごとに最初に77ms 秒でアプリケーションました。 ただし、これらのモデルは、power web search に対して1秒あたり何百万という数のクエリとスニペットを実行する必要があるため、web 検索のスケールでは、推定では77ms でも非常に高額になります。そのため、1つの検索機能を1回だけ発送する必要があります。</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/0784edf6-9167-4a1d-a271-213f22dc62fe.png"><img alt="Bert Model Optimization" border="0" height="530" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/e1500ba4-178c-4118-a6f6-21f114879013.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="Bert モデルの最適化" width="1024"></a></p>


  <h2>Azure 仮想マシンの Gpu を利用して800x 推定スループットを実現する</h2>


  <p>トランスフォーマーと以前の DNN アーキテクチャの主な違いの1つは、シーケンシャル処理ではなく、大規模な並列計算に依存していることです。 GPU (graphics processing unit) アーキテクチャは、スループットの高い並列コンピューティング向けに設計されているため、GPU アクセラレータが組み込まれている Azure &rsquo; s N シリーズ Virtual Machines (VM) は、これらのトランスフォーマーモデルを高速化するために自然に適合していました。 私たちは、 <a href="https://azure.microsoft.com/en-us/pricing/details/virtual-machines/windows/#n-series">NV6 仮想マシン</a> から始めて、コストとリージョンの可用性が低いことを主に決定しました。 &nbsp; GPU を搭載した VM で3層 BERT モデルを実行するだけで、20ミリ秒のサービス待機時間 (約3倍) が見られます。 サービス効率をさらに向上させるために、NVIDIA と提携して、GPU アーキテクチャを最大限に活用し、CUBLAS ライブラリを使用してモデル全体を再実装しています。これには、埋め込み、トランスフォーマー、出力レイヤーの書き直しなどが含まれます。 &nbsp; NVIDIA は、softmax、GELU、正規化、削減など、効率的な CUDA トランスフォーマープラグインも提供しています。</p>


  <p>私たちは、アプリケーションに最適化された GPU モデルを同じ Azure NV6 仮想マシン上に置いています。このモデルでは、スループットの加速を実現するために、8倍の待機時間の高速化と34x スループットの向上を実現しています。 次に、NC6s_v3 仮想マシン上で精度が混合された状態で、さらにパフォーマンスを最適化し、6ミリ秒で64の推論のバッチサイズをベンチマークします (CPU と比較した場合の最大800x スループットの向上)。</p>


  <h2>Azure &rsquo; s global scale を使用して世界中の Bing 検索エクスペリエンスを変革</h2>


  <p>これらの GPU 最適化により、4つのリージョンにわたって2000以上の Azure GPU Virtual Machines を使用して、世界中の1秒あたり100万の BERT 推論を行うことができました。 Azure N シリーズの GPU vm は、高可用性、機敏性、および大幅なコスト削減により、革新的な AI ワークロードと製品品質の Bing 向上を実現する上で重要です。特に、ディープラーニングモデルは複雑になっています。 通じ重要のよう Bing な大規模な組織でも、組み込みの GPU アクセラレーションを使用して Azure 上で N シリーズの仮想マシンを使用することによって、AI ワークロードを加速させることができました。 Gpu を使用せずにこのようなグローバルスケールの AI 推論を提供する場合、CPU ベースの Vm の数が指数関数的に増加する必要があります。これにより、最終的にコストが高くなります。 &nbsp;また、Azure では、複数の種類の Gpu に迅速にデプロイできる機敏性を備えています。これは、オンプレミスで Gpu をインストールした場合に、数か月かかることがあります。 &nbsp;N シリーズの Virtual Machines は、高度なディープラーニングモデルを最適化して出荷することによって、現在世界中で利用できる Bing 検索を向上させるために不可欠でした。</p>


  <h2>N シリーズの一般提供</h2>


  <p>Azure には、 <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-gpu" target="_blank">NC、ND、および NV シリーズ</a> の製品ライン全体にわたる仮想マシンの機能の完全なポートフォリオが用意されています。 これらの Virtual Machines は、コンピューティング集中型、グラフィック集中型、および視覚化ワークロードなど、GPU アクセラレーションが共通するアプリケーションシナリオ向けに設計されています。</p>


  <ul>
      <li>NC シリーズの Virtual Machines は、コンピューティング集中型およびネットワーク集中型のアプリケーション向けに最適化されています。</li>
      <li>ND シリーズの Virtual Machines は、ディープラーニングのトレーニングと推論のシナリオに合わせて最適化されています。</li>
      <li>NV シリーズ Virtual Machines は、視覚化、ストリーミング、ゲーム、エンコーディング、および VDI のシナリオに合わせて最適化されています。</li>
  </ul>


  <p>ND および NV シリーズ Virtual Machines に対する最近の製品の追加については、 <a href="https://azure.microsoft.com/en-us/blog/new-azure-hpc-and-partner-offerings-at-supercomputing-19/" target="_blank">Supercomputing19 ブログ</a> を参照してください。</p>


  <h2>詳細情報</h2>


  <p>Supercomputing19 に参加して、Azure gpu を活用した Bing の最適化に関する詳細をご確認ください。</p>
