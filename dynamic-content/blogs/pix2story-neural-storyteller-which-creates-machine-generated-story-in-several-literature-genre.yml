### YamlMime:Yaml
ms.openlocfilehash: 1a1ddcb0cbad5e7d27d90321925d440d2f690b45
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/11/2022
ms.locfileid: "139904563"
Slug: pix2story-neural-storyteller-which-creates-machine-generated-story-in-several-literature-genre
Title: 'Pix2Story: 複数の文献ジャンルでコンピューターによって生成されたストーリーを作成するニューラルエフェクト'
Summary: 'ストーリーテリングは人間の性質の中核です。 私たちは、お客様が執筆する前に、私たちの値を共有し、ほとんどの場合、組織を使用して伝えるを作成しました。 次に、ストーリーを記録して共有する方法を見つけて、ストーリーを幅広く共有するより高度な方法を見つけています。Gutenberg の印刷から、テレビ、およびインターネットに接続します。 '
Content: >-
  <p>ストーリーテリングは人間の性質の中核です。 私たちは、お客様が執筆する前に、私たちの値を共有し、ほとんどの場合、組織を使用して伝えるを作成しました。 次に、ストーリーを記録して共有する方法を見つけて、ストーリーを幅広く共有するより高度な方法を見つけています。Gutenberg &rsquo; s 印刷から、テレビ、およびインターネットに接続します。 ストーリーを記述するのは簡単ではありません。特に、別の literary のジャンルの画像を見て、ストーリーを記述する必要がある場合は、簡単ではありません。</p>


  <p>自然言語処理 (NLP) は、コンピューター人間との対話によって変革を推進するフィールドです。 私たちは現在、コンピュータービジョンに関して驚くほどの正確な精度を見てきましたが、NLP を紹介するより自然で凝集した話を作成できるかどうかを見てみたいと思いました。 Microsoft では、Azure 上の <a href="https://www.ailab.microsoft.com/experiments/7feda326-186c-494b-9b12-fbb41f1dce0c" target="_blank">Pix2Story</a> web アプリケーションを開発しました。これにより、ユーザーは画像をアップロードし、複数の文献のジャンルに基づいてコンピューターによって生成されたストーリーを取得できます。 私たちは、いくつかのホワイトペーパー &ldquo; の<a href="https://arxiv.org/abs/1506.06726" target="_blank">スキップ</a> &rdquo; &ldquo; 、<a href="https://arxiv.org/abs/1502.03044" target="_blank">表示、参加、および通知を行っています。</a>これは、<a href="https://arxiv.org/abs/1506.06724" target="_blank">映画や読書ブック</a> &rdquo; を見ることでストーリーに似た &rdquo; &ldquo; 視覚的な説明に向けて、また、いくつかのリポジトリを<a href="https://github.com/ryankiros/neural-storyteller" target="_blank">ニューラルエフェクト</a>に合わせています。 ここでは、アップロードした画像からキャプションを取得し、それらを定期的なニューラルネットワークモデルにフィードして、ジャンルと画像に基づいてナレーションを生成します。</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/93d4031b-3517-4b8d-bae5-43369cee7372.gif"><img alt="Pix2Story_Homepage-Carousel_580x326-v3" height="326" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/d32d5d20-914c-443f-97a3-ba2cae8df446.gif" style="margin-right: auto; margin-left: auto; float: none; display: block;" title="Pix2Story_Homepage-Carousel_580x326-v3" width="580"></a></p>


  <h2>解決策</h2>


  <p>プロセスの一部では、アップロードされたイメージを分析してキャプションを生成することによって視覚的な入力を理解するために、300.000 イメージの MS COCO キャプションデータセットに対して視覚的なセマンティック埋め込みモデルをトレーニングしました。 また、キャプションを変換し、選択したジャンル (adventure、Sci-Fi、thriller) に基づいてナレーションを生成しました。 この場合、2000 novels 以上のエンコーダーデコーダーモデルの2週間のトレーニングを行います。 このトレーニングを使用すると、novels の各経路を、ベクトル空間に格納する方法である skip 思考ベクターにマップできます。 これにより、エンコードされた文字の周囲の文を再構築するために、単語だけでなく、これらの単語の意味を理解できるようになりました。 この新しい Azure Machine Learning サービスと azure モデル管理 SDK (Python 3) を使用して、これらのモデルを使用して docker イメージを作成し、azure Kubernetes Services (AKS) と GPU 機能を使用してプロジェクトを運用環境に準備することでデプロイしました。 プロセスフローを詳しく見てみましょう &rsquo; 。</p>


  <h2>視覚的セマンティック埋め込み</h2>


  <p>プロジェクトの最初の部分は、入力画像をキャプションに変換するものです。 キャプションは、次の例に示すように、画像を簡単に説明します。</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/fb636832-f7bc-427d-b151-ff2d49107984.jpg"><img alt="White dog sitting on green grass" border="0" height="461" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/72d075d0-d642-4e50-9433-6b9253b63dc7.jpg" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="緑色の草に座ったホワイトドッグ" width="692"></a></p>


  <p><strong>ホワイトドッグは、緑と白の弓を持つ Frisbee 小型の白い犬が、黒色のスポットで白い犬に関連しています。</strong></p>


  <p>このキャプションの生成に使用されるモデルは、2つの異なるネットワークによって構成されます。 1つ目は、注釈ベクトルと呼ばれる特徴ベクトルのセットを抽出する畳み込みニューラルネットワークです。</p>


  <p>モデルの2番目の部分は、長い短期メモリ (LSTM) ネットワークです。これは、コンテキストベクター、以前に非表示にした状態、および以前に生成された単語に対して、毎回1単語を生成することによってキャプションを生成します。</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/d620ee10-ec6b-4f4f-8c4e-68393ba70562.jpg"><img alt="Feature map" border="0" height="272" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/8f3becff-13ad-4e11-91ce-a8239b525e25.jpg" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="機能マップ" width="613"></a></p>


  <h2>Skipthought ベクター</h2>


  <p>Kiros による skipthought ベクターは、さまざまなタスクで使用できる汎用センテンス表現を生成するモデルです。 このプロジェクトでは、ブックからのテキストの継続性を使用して、エンコードされた文字の周囲のセンテンスの再構築を試みるエンコーダーデコーダーモデルをトレーニングします。</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/c097a677-970b-4c8d-b8ea-dcca66d266f9.jpg"><img alt="Skipthought vectors" border="0" height="152" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/52ac209e-0965-46df-ae7f-41a01247a7a9.jpg" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="Skipthought ベクター" width="873"></a></p>


  <p>モデルは、エンコーダーデコーダーモデルです。 エンコーダーは、文をベクターにマップするために使用されます。 デコーダーは、このベクターを条件として、ソース文の翻訳を生成します。</p>


  <p>使用されるボキャブラリは google news の事前トレーニング済みベクトルを使用して拡張されています。これは、書籍の語彙に含まれる単語をこのベクトル内の単語にマップする線形リグレッサーを生成することで行われます。</p>


  <h2>スタイルシフト</h2>


  <p>説明文が機能していない場合は、その内容が簡潔であることがわかります。最後の出力は短い文です。 そのため、必要な出力が literary になる場合は、スタイルをシフトさせる必要があります。 これは、skipthought ベクター表現を操作して、出力で実行する特性に入力を設定することを意味します。 操作は次のとおりです。</p>


  <p>Skipthoughts デコーダーの入力 = エンコードされたすべてのキャプションのキャプションエンコードされたキャプションを、予想される出力と同様の長さと特徴でエンコードします。</p>


  <h2>デプロイ</h2>


  <p>このプロジェクトは Azure Machine Learning サービスワークスペースを使用してデプロイされており、ファイルと、予測に関係するすべてのモデルを使用して Docker イメージを生成します。 AKS を使用して、ソリューションを自動的にスケーリングするためのデプロイが行われています。</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/091fbe04-5dc9-4b47-8674-fdda13b26fa6.png" target="_blank"><img alt="image" border="0" height="553" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/5b2dafee-b54e-4785-829f-e9dc9bccce0c.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="絵" width="1024"></a></p>


  <h2>独自のモデルをトレーニングする</h2>


  <h3>新しいモデルのトレーニング:</h3>


  <ul>
   <li>Conda 環境の作成:</li>
  </ul>


  <pre>

  conda env create --file environment.yml</pre>


  <ul>
   <li>Conda env をアクティブにします。</li>
  </ul>


  <pre>

  activate storytelling</pre>


  <ul>
   <li>Config.py のブックまたはテキストにパスを設定し、トレーニングの設定を行います。</li>
   <li>Training.py を実行して、エンコーダーをトレーニングし、必要なファイルを生成し、テキストに基づいてデコーダーをトレーニングします。</li>
   <li>[バイアスの生成]: エンコードされた文の平均と、エンコードされたキャプションの平均を表します。</li>
   <li>ストーリーを生成するには、python コンソールで次のように実行します。</li>
  </ul>


  <pre>

  &gt;import generate

  &gt;story_generator = generate.StoryGenerator()

  &gt;story_generator.story(image_loc=&#39;path/to/your/image&#39;)</pre>


  <p>おめでとうございます。 これで、作業を開始するための完全に動作するアプリケーションが完成しました。 プロジェクトのテストを楽しんで、投稿をありがとうございました。</p>


  <p>&quot;Pix2Story-ニューラルエフェクト &quot; -ユーザーが画像をアップロードし、複数の literary ジャンルに基づいて AI で生成されたストーリーを取得できるようにする web アプリ。</p>


  <h2>その他の技術情報</h2>


  <ul>
   <li><a href="https://aka.ms/pix2story" target="_blank">AI ラボ</a></li>
   <li><a href="https://pix2story.azurewebsites.net" target="_blank">プレイグラウンド</a></li>
  </ul>


  <p>コード、ソリューション開発プロセス、および<a href="https://github.com/Microsoft/ailab/tree/master/Pix2Story" target="_blank">GitHub</a>に関するその他のすべての詳細を確認できます。</p>


  <p>この投稿は AI の使用を開始するのに役立ちます。 AI 開発者になることをためています。</p>
