### YamlMime:Yaml
ms.openlocfilehash: c5e6eb39e64ab02993cd6ce66f4b69a846619b7d
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/11/2022
ms.locfileid: "139905183"
Slug: onnx-runtime-is-now-open-source
Title: ONNX ランタイムがオープンソースになりました
Summary: 現時点では、オープンソースのオープンニューラルネットワーク Exchange (ONNX) ランタイムが GitHub に発表されています。 ONNX Runtime は、Linux、Windows、Mac の ONNX 形式での機械学習モデルのための高パフォーマンスの推論エンジンです。
Content: >-
  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/9f6e2845-0f22-47a1-a038-0fd3a97b1c2b.png"><img align="left" alt="ONNX Runtime Logo" border="0" height="139" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/738e9a46-5e21-4251-b8bf-992529c25468.png" style="border: 0px currentcolor; border-image: none; float: left; display: inline; background-image: none;" title="ONNX ランタイムのロゴ " width="247"> </a> 現在は、オープンソースオープンニューラルネットワーク Exchange (ONNX) ランタイムが <a href="https://github.com/microsoft/onnxruntime" target="_blank">GitHub</a>に発表されています。 ONNX Runtime は、Linux、Windows、Mac の ONNX 形式での機械学習モデルのための高パフォーマンスの推論エンジンです。</p>


  <p><a href="https://onnx.ai/" target="_blank">ONNX</a>  は、Microsoft が Facebook および AWS と共同で開発したディープラーニングおよび従来の機械学習モデルのオープン形式です。 ONNX 形式は、AI を使いやすくし、価値を高めるためのオープンエコシステムの基礎となります。開発者は、タスクに適したフレームワークを選択できます。また、フレームワークの作成者は革新的な機能強化に専念でき、ハードウェアベンダーはニューラルネットワーク計算の最適化を効率化することができます。 </p>


  <p>Microsoft は、3年以上にわたり AI で研究を行ってきましたが、機械学習と深いニューラルネットワークを多数の製品やサービスに組み込んでいます。 さまざまなトレーニングフレームワークを使用し、さまざまなデプロイオプションを対象とするチームでは、これらの分散型ソリューションを統合して、運用化モデルをすばやく簡単に作成できるようにする必要がありました。 ONNX ランタイムはそのソリューションを提供します。 これにより、データ科学者は、選択したフレームワークでモデルをトレーニングおよびチューニングする柔軟性が得られ、クラウドとエッジの両方にまたがる製品の高パフォーマンスでこれらのモデルをとすることができます。</p>


  <h2>ONNX Runtime を使用する理由</h2>


  <p>ONNX Runtime は、ML ONNX プロファイルを含む ONNX 1.2 以降を完全にサポートする、一般公開されている推定エンジンです。 これは、AI モデルと技術頼っの進化したセットをサポートするために、ONNX 標準と共に直接進めていることを意味します。</p>


  <p>Microsoft では、チームは ONNX ランタイムを使用して、Bing Search、Bing 広告、Office 生産性サービスなどの主要なシナリオで使用される多くのモデルのスコアリング待機時間と効率を向上させています。 ONNX に変換&#39;たモデルでは、既存のソリューションでのスコアリングと比較して、平均パフォーマンスが2倍に向上している&#39;ます。 ONNX Runtime は、Windows ML や ML .net など、その他の Microsoft 製品にも組み込まれています。</p>


  <p>ONNX ランタイムは軽量でモジュール形式の設計になっており、CPU のビルドには数 mb のサイズがあります。 <a href="https://github.com/Microsoft/onnxruntime/blob/master/docs/HighLevelDesign.md" target="_blank">拡張可能なアーキテクチャ</a>   を使用すると、オプティマイザーとハードウェアアクセラレータは、実行プロバイダーとし &ldquo; て登録することにより、計算の待機時間を短くし、効率を高めることができます。 &rdquo;結果として、エンドツーエンドのユーザーエクスペリエンスが向上し、待機時間が短くなり、コンピューターの使用率とスループットが低下することによるコスト削減にもつながります。 </p>


  <h2>業界パートナーからのディープサポート</h2>


  <p>ONNX コミュニティの大手企業は、ONNX Runtime とテクノロジを統合するために積極的に取り組んでいるか、計画を立てています。 これにより、最高のパフォーマンスを実現しながら、ONNX の完全な仕様をサポートすることができます。</p>


  <p>Microsoft と Intel は連携して、nGraph コンパイラを ONNX ランタイムの実行プロバイダーとして統合しています。 NGraph コンパイラは、デバイス固有の最適化とデバイス固有の最適化の両方を適用することによって、既存のハードウェアターゲットと将来のハードウェアターゲットの両方を高速化することができます。CPU の推定に nGraph コンパイラを使用すると、ネイティブフレームワークと比較して最大<a href="https://ai.intel.com/ngraph-compiler-stack-beta-release/" target="_blank">45x 倍のパフォーマンスが向上</a>   します。  </p>


  <p>NVIDIA は、ONNX Runtime との統合によって、可能な限り最高のパフォーマンスを実現しながら、急速に拡大している一連のモデルとアプリを NVIDIA Gpu にデプロイするための簡単なワークフローを提供しています。NVIDIA の "recommenders"、"自然言語の処理"、"画像/ビデオ処理" などのアプリケーション間で、最小限の待機時間で大幅に高いスループットを実現する、高パフォーマンスの推論オプティマイザーとランタイムが含まれています。 </p>


  <p>もう1つ   の ONNX の Qualcomm は、ONNX <a href="https://developer.qualcomm.com/blog/run-your-onnx-ai-models-faster-snapdragon" target="_blank">Runtime のサポート</a>も表しています。&ldquo;ONNX Runtime の導入は、フレームワークの相互運用性、標準化、およびパフォーマンスの最適化を複数のデバイスカテゴリにわたって促進するための前向きな一歩です。開発者は、ONNX Runtime &quot; for The Brotman, The, Qualcomm Technologies, inc. での AI 製品管理のシニアディレクターについてのサポートを開始します。  </p>


  <p>ONNX を最近参加させた後、ONNX ランタイムのサポートも発表されました。&ldquo;多くの機械学習フレームワークの中から選択する場合は、お客様に対して、NXP での AI テクノロジセンターの氏によりとして、最大限の柔軟性と自由 &rdquo; 度を設定することをお勧めします。&quot;&rsquo;Microsoft のプラットフォームでリリースされた ONNX ランタイムをサポートすることで、ML 開発者のお客様コミュニティに ONNX 特典を提供します。&rdquo;</p>


  <p>ハードウェアパートナーに加えて、フレームワークプロバイダーの優先ネットワークも ONNX ランタイムを利用しています。 &quot;ディープラーニングフレームワーク <a href="https://chainer.org/" target="_blank">Chainer</a>の開発に加えて、 <a href="https://github.com/pfnet-research/menoh" target="_blank">優先される</a>ネットワークは、複数のプログラミング言語 &quot; 用の ONNX 推論エンジンラッパーライブラリであり、Nishikawa、社長、および優先ネットワーク (inc.) の CEO であると述べています。 &quot;Menoh はメインバックエンドとして ONNX Runtime を使用し、Chainer は現在 ONNX Runtime を使用して ONNX エクスポート機能をテストします。 優先されるネットワークは、Microsoft が ONNX Runtime を喜びし、今後 Microsoft と ONNX を連携させることができるようになりました。&quot;  </p>


  <h2>ONNX ランタイムの使用方法</h2>


  <p>まず&#39;、ONNX モデルが必要です。 Don&#39;ONNX モデルはありますか。 問題はありません。 ONNX の利点は、 <a href="https://github.com/onnx/tutorials" target="_blank">さまざまなツール</a>でフレームワークの相互運用性が実現されていることです。</p>


  <ul>
   <li><a href="https://github.com/onnx/models" target="_blank">ONNX モデルの Zoo</a>から直接、resnet やなどの一般的なモデルの事前トレーニング済みバージョンを直接取得できます。</li>
   <li><a href="https://docs.microsoft.com/en-us/azure/cognitive-services/Custom-Vision-Service/home" target="_blank">Azure Custom Vision 認知サービス</a>を使用して、カスタマイズされた独自のコンピュータービジョンモデルを作成できます。</li>
   <li>既に Scikit-learn、または CoreML 形式のモデルがある場合は、オープンソースのコンバーター (<a href="https://pypi.org/project/onnxmltools/" target="_blank">ONNXMLTools</a> と <a href="https://pypi.org/project/tf2onnx/" target="_blank">TF2ONNX</a>) を使用して変換できます。</li>
   <li>Azure Machine Learning サービスを使用して新しいモデルをトレーニングし、 <a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-build-deploy-onnx" target="_blank">ONNX 形式に保存</a>することができます。  </li>
  </ul>


  <p>ONNX ランタイムを使用するには、目的のプラットフォーム用のパッケージと選択した言語をインストールするか、ソースからビルドを作成します。 ONNX ランタイムは、Linux、Windows、および Mac と互換性のある <strong>Python</strong>、 <strong>C#</strong>、および<strong>C</strong>インターフェイスでの <strong>CPU </strong>と <strong>GPU </strong>(cuda) の両方をサポートしています。インストール手順については、 <a href="https://github.com/microsoft/onnxruntime" target="_blank">GitHub</a>を確認してください。     </p>


  <p>ONNX Runtime は、source から直接、またはプリコンパイル済みバイナリからコードに統合することができますが、アプリケーションがを呼び出すための <a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-build-deploy-onnx" target="_blank">サービス</a>   をデプロイするために、Azure Machine Learning を使用するのが簡単な方法です。   </p>


  <h2>参加する  </h2>


  <p>ONNX ランタイムのリリースでは、AI 向けにオープンで相互運用可能なエコシステムへの取り組みの重要な手順を示しています。これまでに、コミュニティからの小学校とサポートについて非常に興奮しています。 AI で製品イノベーションを推進し、開発コミュニティで試してみることを強くお勧めします。ONNX ランタイムを継続的に進化させ、改善しています。お客様からのフィードバックと、この非常に魅力的な分野への投稿をお待ちしております。  </p>


  <p>フィードバックまたは質問がある場合は、 <a href="https://github.com/Microsoft/onnxruntime/issues" target="_blank">問題</a>   のファイル<a href="https://github.com/microsoft/onnxruntime" target="_blank">Github</a>で、 <a href="https://twitter.com/onnxruntime">Twitter</a>でフォローします。  </p>
