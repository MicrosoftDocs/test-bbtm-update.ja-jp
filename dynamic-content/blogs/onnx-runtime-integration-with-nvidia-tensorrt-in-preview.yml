### YamlMime:Yaml
ms.openlocfilehash: 006bc8b2d5018fe115e2c1bc18411293b39a195e
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/11/2022
ms.locfileid: "139905186"
Slug: onnx-runtime-integration-with-nvidia-tensorrt-in-preview
Title: プレビューでの ONNX Runtime と NVIDIA TensorRT の統合
Summary: '本日は、ONNX Runtime の NVIDIA TensorRT 実行プロバイダーのプレビューをオープン ソースにしています。 このリリースでは、開発者が選択したフレームワークに関係なく、業界をリードする GPU アクセラレーションを簡単に活用することで、オープンで相互運用可能な AI に向けた別の一歩を踏み出しています。 '
Content: >-
  <p>本日は、ONNX Runtime の NVIDIA TensorRT 実行プロバイダーのプレビューをオープン <a href="https://azure.microsoft.com/en-us/blog/onnx-runtime-is-now-open-source/">ソースにしています</a>。 このリリースでは、開発者が選択したフレームワークに関係なく、業界をリードする GPU アクセラレーションを簡単に活用することで、オープンで相互運用可能な AI に向けた別の一歩を踏み出しています。 開発者は、ONNX Runtime を介して <a href="https://developer.nvidia.com/tensorrt">TensorRT</a> の機能を利用して、PyTorch、TensorFlow、その他多くの一般的なフレームワークからエクスポートまたは変換できる ONNX モデルのインフェレンシングを加速できます。</p>


  <p>Microsoft と NVIDIA は、TensorRT 実行プロバイダーと ONNX Runtime の統合に密接に取り組み、モデル Zoo 内のすべての ONNX モデルのサポートを <a href="https://github.com/onnx/models">検証しました</a>。 TensorRT 実行プロバイダーでは、ONNX ランタイムは、汎用 GPU アクセラレーションと比較して、同じハードウェアで優れたインフェレンシング パフォーマンスを提供します。 MultiMedia サービスの内部ワークロードで TensorRT 実行プロバイダーを使用して、最大 2 倍のパフォーマンスBing確認しました。</p>


  <h2>しくみ</h2>


  <p>ONNX Runtime とその TensorRT 実行プロバイダーは、グラフを解析し、サポートされているハードウェアで TensorRT スタックによって実行される特定のノードを割り当てると、ディープ ラーニング モデルのインフェレンシングを加速します。 TensorRT 実行プロバイダーは、プラットフォームにプレインストールされている TensorRT ライブラリとインターフェイスを使用して、ONNX サブグラフを処理し、NVIDIA ハードウェア上で実行します。 これにより、開発者はさまざまな種類のハードウェアにわたって ONNX モデルを実行し、さまざまなハードウェア構成を柔軟にターゲットとするアプリケーションを構築できます。 このアーキテクチャは、ディープ ニューラル ネットワークの実行を最適化するために不可欠なハードウェア固有のライブラリの詳細を抽象化します。</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/36f70acc-f94f-4c42-82c8-98b7e1e22533.png"><img alt="Infographic showing input data and output result using the ONNX model" border="0" height="483" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/67b71a1a-0165-453c-bc2b-9fe997c603c3.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="ONNX モデルを使用した入力データと出力結果を示すインフォグラフィック" width="1024"></a></p>


  <h2>TensorRT 実行プロバイダーを使用する方法</h2>


  <p>ONNX Runtime と TensorRT 実行プロバイダーは、Opset のバージョン 9 を使用して ONNX Spec v1.2 以上をサポートしています。 TensorRT 最適化モデルは、Azure 上の NVIDIA GPU を搭載しているすべての N シリーズ VM にデプロイできます。</p>


  <p>TensorRT を使用するには、まず TensorRT 実行プロバイダーを使用 <a href="https://github.com/Microsoft/onnxruntime/blob/master/BUILD.md">して ONNX</a> ランタイムをビルドする必要があります (<code>&nbsp;--use_tensorrt --tensorrt_home &lt;path to </code><code>location</code><code> for TensorRT libraries in your local machine&gt;</code> build.sh ツールでフラグを使用します)。 その後、ONNX ランタイム API を使用して推論セッションを開始することで、TensorRT を利用できます。 ONNX Runtime は、パフォーマンスを最大化するために、TensorRT による実行に適したサブグラフの優先順位を自動的に設定します。</p>


  <pre>

  InferenceSession session_object{so};

  session_object.RegisterExecutionProvider(std::make_unique&lt;::onnxruntime::TensorrtExecutionProvider&gt;());

  status = session_object.Load(model_file_name);</pre>


  <p>詳細な手順については、<a href="https://aka.ms/trt-onnxrt">GitHub。</a> さらに、TensorRT 実行プロバイダーを使用した ONNX ランタイム ビルドの検証に役立つ、レポポの onnx_test_runner ユーティリティを通じて標準テストのコレクションを使用できます。</p>


  <h2>ONNX および ONNX ランタイムとは</h2>


  <p><a href="https://onnx.ai/">ONNX</a> は、Microsoft が Facebook および AWS と共同開発したディープ ラーニングと従来の機械学習モデル用のオープン形式です。 ONNX を使用すると、ONNX Runtime を使用してさまざまなハードウェア プラットフォームで実行できる一般的な形式でモデルを表現できます。 これにより、開発者はタスクに適切なフレームワークを自由に選択できるだけでなく、選択したハードウェアを使用してさまざまなプラットフォームでモデルを効率的に実行できる自信が得ます。</p>


  <p><a href="https://github.com/microsoft/onnxruntime">ONNX ランタイムは</a>、ONNX 1.2 以上を完全にサポートする、ONNX-ML プロファイルを含む、最初の一般公開推論エンジンです。 ONNX Runtime は軽量でモジュール式で<a href="https://github.com/Microsoft/onnxruntime/blob/master/docs/HighLevelDesign.md"></a>、TensorRT &ldquo;などのハードウェア アクセラレータを実行プロバイダーとしてプラグインできる拡張可能なアーキテクチャです。&rdquo;これらの実行プロバイダーは、低待機時間と高効率のニューラル ネットワーク計算のロックを解除します。 現在、ONNX Runtime は、数十億人のユーザーにサービスを提供する主要なシナリオを、Bing、Officeに提供しています。</p>


  <h2>オープンで相互運用可能な AI に向けたもう 1 つのステップ</h2>


  <p>ONNX Runtime の TensorRT 実行プロバイダーのプレビューは、AI 用のオープンで相互運用可能なエコシステムを作成するために、もう 1 つのマイルストーンを示しています。 これにより、実稼働モデルの待ち時間要件が増え続け、世界で AI イノベーションを促進しやすくなります。 ONNX ランタイムは継続的に進化および改善され、フィードバックと投稿をお待ちしています。  </p>


  <p>クラウドとエッジでの高速なインフェレンシングに ONNX を使用する方法の詳細については、NVIDIA GTC の <a href="https://gputechconf2019.smarteventscloud.com/connect/search.ww#loadSearch-searchPhrase=ONNX&amp;searchType=session&amp;tc=0&amp;sortBy=dayTime&amp;p=">ONNX</a> セッションを確認してください。 ONNX ランタイムに関するフィードバックや質問はありますか? <a href="https://github.com/Microsoft/onnxruntime/issues">このページで問題</a>をGitHub Twitter でフォロー<a href="https://twitter.com/onnxruntime">してください</a>。&nbsp;</p>
