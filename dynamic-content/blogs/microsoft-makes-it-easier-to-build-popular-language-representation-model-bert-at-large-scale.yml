### YamlMime:Yaml
ms.openlocfilehash: 8953ebdd6d446a3d2fdabb931f6b9e69845c6b28
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/11/2022
ms.locfileid: "139905237"
Slug: microsoft-makes-it-easier-to-build-popular-language-representation-model-bert-at-large-scale
Title: Microsoft では、一般的な言語表現モデル BERT を大規模に簡単に構築できます
Summary: 現在、Bing チームによって構築された BERT (トランスフォーマーからの双方向エンコーダー表現) を事前トレーニングするためのレシピのオープン ソーシングを発表します。Azure Machine Learning で動作するコードも含めて、お客様は組織の BERT 大規模モデルのカスタム バージョンをトレーニングする機能を利用できます。 これにより、開発者とデータ サイエンティストは、BERT を超えて独自の汎用言語表現を構築できます。
Content: >-
  <p><em>この投稿は、Rangan Maximumder、Group Program Manager、Bing、Maxim Lukiyanov (プリンシパル プログラム マネージャー、 Azure Machine Learning) によって共同執筆されています。</em></p>


  <p>本日は、Bing チームによって構築された BERT (トランスフォーマーからの双方向エンコーダー表現) を事前トレーニングするためのレシピのオープン ソーシングを発表します。Azure Machine Learning で動作するコードも含めて、お客様は<a href="https://azure.microsoft.com/en-us/services/machine-learning-service/"></a>独自のデータを使用して BERT 大規模モデルのカスタム バージョンをトレーニングする機能を利用できます。 これにより、開発者とデータ サイエンティストは、BERT を超えて独自の汎用言語表現を構築できます。</p>


  <p>自然言語処理の領域では、過去数年間で驚くべき量のイノベーションが見られます。最新の BERT の 1 つが BERT です。<a href="https://arxiv.org/abs/1810.04805"> Google</a> AI 言語調査によって作成された言語表現である BERT は、言語の重要な機能をキャプチャする機能を大幅に進歩し、テキストの分類、抽出、質問の回答など、多くの自然言語アプリケーションの最新の技術を向上しました。 この新しい言語表現を作成することで、開発者やデータ サイエンティストは、BERT をステップインタスクとして使用して、特殊な言語タスクを解決し、自然言語処理システムをゼロから構築する場合よりもはるかに優れた結果を得ることができます。</p>


  <p>BERT の広範な適用性は、ほとんどの開発者やデータ サイエンティストが、新しいデータを使用して新しいバージョンを一から構築するのではなく、事前トレーニング済みの BERT バリアントを使用できるという意味です。 ドメイン データが元&rsquo;&rsquo;のモデル データと似ている場合、これは妥当な解決策ですが、新しい問題空間に渡る際にクラス最高の精度を提供する必要はありません。 たとえば、医療ノートの分析のためのモデルをトレーニングするには、医療分野について深く理解する必要があります。キャリアの推奨事項は、ジョブと候補に関する大量のテキストコーパスからの分析情報に依存し、法的ドキュメント処理には法的ドメイン データに対するトレーニングが必要です。 このような場合、自然言語処理 (NLP) アルゴリズムの精度を最大化するには、微調整を超えて BERT モデルを事前トレーニングする必要があります。</p>


  <p>さらに、BERTS&rsquo; の精度を超えて言語表現を進め、ユーザーはモデル アーキテクチャ、トレーニング データ、コスト関数、タスク、最適化ルーチンを変更する必要があります。 これらの変更はすべて、大規模なパラメーターとトレーニング データ サイズで探索する必要があります。 BERT-large の場合、これは 3 億 4,000 万のパラメーターを持ち、25 億 Wikipedia と 8 億件の BookCorpus 単語を超えるトレーニングを行ったので、非常に大きな可能性があります。 これを、ディープ ラーニング ベースの NLP モデルのトレーニングに使用される最も一般的なハードウェアであるグラフィカル処理ユニット (GPU) でサポートするには、機械学習エンジニアがこれらの大規模なモデルをトレーニングするために分散トレーニング サポートが必要になります。 ただし、これらの分散環境を構成する複雑さとフラグリティにより、専門家による調整でも、トレーニング済みのモデルから得られる結果が得られる可能性があります。</p>


  <p>これらの問題に対処するために、Microsoft は、Azure で BERT 大規模モデルのカスタム バージョンをトレーニングするために、最初の種類のエンドツーエンドのレシピをオープン ソーシングしています。 全体的に、これは安定した予測可能なレシピであり、開発者やデータ サイエンティストが独自に探索を試すのに最適です。</p>


  <p><em>&ldquo;BERT の微調整は、Bing&rdquo; 検索の関連性に重要なさまざまなタスクの品質を向上させるために非常に役立ちます。この作業のオープン ソーシングを主導した Bing&nbsp; &ldquo; のグループ プログラム マネージャー、Rangan Relevancumder は言います。しかし、基になるデータが元のコーパス BERT と異なるいくつかのタスクが事前トレーニングされ、タスクとモデル アーキテクチャの変更を試してみたいと思いました。&nbsp;これらの探索を可能にするために、私たちの科学者と研究者のチームは、GPU で BERT を事前トレーニングする方法を解決するために取り組んでいました。その後、改善された表現を構築し、BERT よりも内部タスクの精度を大幅に向上させる可能性があります。&nbsp;コミュニティがエクスペリエンスをレプリケートし、そのニーズを満たす新しい方向に拡張する力を提供するために、Bing で行った作業をオープン ソースにしています。&rdquo;</em></p>


  <p><em>&ldquo;GPU 上の元の BERT&rdquo; リリースと同じ品質に収束するトレーニングを得るのは簡単ではありません。「Saurabh T convergry(Bing&nbsp; &ldquo; の Applied Science Manager)」と言います。BERT を事前トレーニングするには、大規模な計算とメモリが必要です。つまり、計算を複数の GPU に分散する必要があります。ただし、最終的に得られるモデルの収束と品質の観点から、予測可能な動作を使用してコスト効率と効率的な方法で行うのは非常に困難でした。分散&rsquo;トレーニング プロセスを簡略化するために行った作業をリリースし、他のユーザーが作業の恩恵を受け取る可能性があります。&rdquo;</em></p>


  <h2>結果</h2>


  <p>コードをテストするために、標準データセットで BERT 大規模モデルをトレーニングし、表 1 に示すように、一連の GLUE タスクで元の論文の結果を再現しました。 必要なコンピューティングの見積もりを提供するために、このケースでは、テーブルに記載されている精度に達するために、8xND40_v2 ノード (合計 64 NVidia V100 GPU) の Azure ML クラスターでトレーニングを 6 日間実行しました。 表示される実際の数値は、データセットと、アップストリーム タスクに使用する BERT モデル チェックポイントの選択によって異なります。</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/63711425-5239-457a-9f91-db16a492f2b3.gif"><img alt="clip_image002" border="0" height="229" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/18d3c1c7-ae4a-4585-889c-3fb721c77ef3.gif" style="border: 0px currentcolor; margin-right: auto; margin-left: auto; display: block; background-image: none;" title="clip_image002" width="912"></a></p>


  <p align="center"><em>Table1。GLUE 開発セットの結果。Google BERT の結果は、開発セットで公開されている BERT モデルを使用して評価されます。平均 &ldquo;列は&rdquo; 、テーブルの結果に対する単純な平均です。QQP と MRPC に対して F1 スコアが報告され、スピアマン相関が STS-B に対して報告され、他のタスクの精度スコアが報告されます。データセット サイズが小さいタスクの結果には大きなバリエーションがあります。結果を再現するには、複数の微調整の実行が必要な場合があります。</em></p>


  <p align="left">このコードは、BERT と BERT のAzure Machine Learning<a href="https://github.com/microsoft/AzureML-BERT">ソースGitHubできます</a>。 このレポポには次の情報が含まれます。</p>


  <ul>
      <li><a href="https://github.com/huggingface/pytorch-pretrained-BERT">Hugging Face</a> レポジターからの BERT モデルの PyTorch 実装。</li>
      <li>未加工および前処理済みの English Wikipedia データセット。</li>
      <li>データ準備スクリプト。</li>
      <li>勾配累積や混合精度などの最適化手法の実装。</li>
      <li>モデル<a href="https://azure.microsoft.com/en-us/services/machine-learning-service/">Azure Machine Learning</a>トレーニングを開始する、Jupyter Notebook のサービスです。</li>
      <li>微調整実験で使用できる事前トレーニング済みのモデルのセット。</li>
      <li>微調整実験を実行するためのノートブックを使用したコード例。</li>
  </ul>


  <p>単純な [すべて&ldquo;&rdquo;実行] コマンドを使用すると、開発者とデータ サイエンティストは、提供されている Jupyter Notebook を使用して独自の BERT <a href="https://azure.microsoft.com/en-us/services/machine-learning-service/">モデルを</a>トレーニングAzure Machine Learningできます。 コード、データ、スクリプト、およびツールは、他のトレーニング環境でも実行できます。</p>


  <h2>まとめ</h2>


  <p>これらの結果を達成するには、私たちの前に研究者のすばらしい仕事を活用する必要がありました。また、コミュニティが作業を進め、さらに進められたらと考えています。 ご質問やフィードバックがある場合は、GitHub のレポポ<a href="https://github.com/microsoft/AzureML-BERT"></a>にアクセスし、改善方法をお知らせください。</p>


  <p>機械学習モデル<a href="https://azure.microsoft.com/en-us/services/machine-learning-service/">Azure Machine Learning</a>の構築、トレーニング、デプロイを効率化する方法について説明します。 <a href="https://azure.microsoft.com/en-us/free/services/machine-learning/">今すぐ無料で開始してください</a>。</p>
