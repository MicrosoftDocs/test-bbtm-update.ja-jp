### YamlMime:Yaml
ms.openlocfilehash: 2bf9b277b523c7654994cda5391087260c9ebe51
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/11/2022
ms.locfileid: "139905190"
Slug: onnx-runtime-for-inferencing-machine-learning-models-now-in-preview
Title: 推論 machine learning モデルの ONNX Runtime を現在プレビュー中
Summary: ONNX 形式の機械学習モデルのための高パフォーマンスの推定エンジンである Open ニューラル Network Exchange (ONNX) Runtime のプレビュー版をリリースします。
Content: "<p><a href=\"https://onnx.ai/\" target=\"_blank\">オープンニューラルネットワーク Exchange (ONNX)</a>形式の機械学習モデルのための高パフォーマンスの推定エンジンである ONNX Runtime のプレビュー版をリリースします。 ONNX Runtime は ONNX バージョン1.2 と互換性があり、 <a href=\"https://pypi.org/project/onnxruntime/\" target=\"_blank\">CPU</a>と<a href=\"https://pypi.org/project/onnxruntime-gpu\" target=\"_blank\">GPU</a>の両方をサポートする Python パッケージに付属しています。これは、 <a href=\"https://azure.microsoft.com/en-us/blog/what-s-new-in-azure-machine-learning-service/\" target=\"_blank\">Azure Machine Learning サービス</a>と Ubuntu 16 を実行しているすべての Linux コンピューターで推論を有効にするためのものです。</p>\n\n<p>ONNX は、ディープラーニングと従来の機械学習のためのオープンソースモデル形式です。 ONNX は2017年12月に開始されたため、業界で20を超える大手企業からのサポートを獲得しています。 ONNX を使用すると、データ科学者や開発者は、自分の作業に適したフレームワークを自由に選択できるだけでなく、選択したハードウェアを備えたさまざまなプラットフォームでモデルを効率的に実行することができます。</p>\n\n<p><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/a811fb6a-c4eb-478c-b2e2-c94f710c7e9d.png\"><img alt=\"ONNX\" border=\"0\" height=\"355\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/094b1b5f-5e21-483b-927d-f5d76422e9c1.png\" style=\"margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;\" title=\"ONNX\" width=\"1176\"></a></p>\n\n<p>ONNX ランタイム推論エンジンは、ONNX で定義されているすべての演算子の包括的なカバレッジとサポートを提供します。 拡張性とパフォーマンスを考慮して開発された、プラットフォームとハードウェアの選択に基づいてさまざまなカスタムアクセラレータを利用し、コンピューティングの待機時間とリソースの使用量を最小限に抑えています。 モデル内で定義されているプラットフォーム、ハードウェア構成、および演算子を使用すると、ONNX Runtime は最も効率的な実行プロバイダーを利用して、推論の全体的なパフォーマンスを最大限に引き出すことができます。</p>\n\n<p>実行プロバイダーのプラグ可能なモデルを使用すると、ONNX Runtime を新しいソフトウェアやハードウェアの進歩に迅速に適応させることができます。 実行プロバイダーインターフェイスは、ハードウェアアクセラレータが ONNX ランタイムに機能を公開するための標準的な方法です。 Intel や NVIDIA をはじめとする企業とのコラボレーションがアクティブになっているため、ONNX ランタイムは専用のハードウェア上のコンピューティングアクセラレーション用に最適化されています。 これらの実行プロバイダーの例としては、Intel&#39;s MKL-DNN と nGraph のほか、NVIDIA&#39;s 最適化されたフル機能があります。</p>\n\n<p><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/228d22d3-6e3e-48b1-811c-1d48353f031c.png\"><img alt=\"ONNXModel\" border=\"0\" height=\"563\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/e7373906-1a6d-425c-8c03-da4bffd47fbb.png\" style=\"margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;\" title=\"ONNXModel\" width=\"1193\"></a></p>\n\n<p>ONNX Runtime のリリースは、Microsoft&#39;s の既存の ONNX のサポートによって拡張され、さまざまなプラットフォームやデバイスで推論の ONNX モデルを実行できるようになりました。</p>\n\n<p><strong>Azure:</strong>ONNX Runtime Python パッケージを使用すると、azure Container Instance または実稼働規模の azure Kubernetes サービスとして Azure Machine Learning を使用して、ONNX モデルをクラウドにデプロイできます。 開始するには、次の <a href=\"https://aka.ms/onnxnotebooks\" target=\"_blank\">例</a> を参照してください。</p>\n\n<p><strong>.Net:</strong> &nbsp;<a href=\"https://www.microsoft.com/net/apps/machinelearning-ai/ml-dotnet\" target=\"_blank\">ML .net</a>を使用して、ONNX モデルを .net アプリに統合できます。</p>\n\n<p><strong>Windows デバイス:</strong> 2018 年10月の最新 Windows 10 更新プログラムで利用可能な組み込みの<a href=\"https://docs.microsoft.com/en-us/windows/ai/\" target=\"_blank\">Windows Machine Learning</a> api を使用して、さまざまな Windows デバイスで ONNX モデルを実行できます。</p>\n\n<p><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/f9f13ccb-d84f-484a-a7c2-40d934e58635.png\"><img alt=\"CreateDeploy\" border=\"0\" height=\"613\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/13189eea-5f0f-4ef9-83d1-92679b3a60f6.png\" style=\"margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;\" title=\"CreateDeploy\" width=\"1271\"></a></p>\n\n<h2>ONNX の使用</h2>\n\n<h3>ONNX モデルを取得する</h3>\n\n<p>ONNX モデルの取得は簡単です。 <a href=\"https://github.com/onnx/models\" target=\"_blank\">ONNX Model Zoo</a>で人気のある事前トレーニング済みの ONNX モデルを選択し、Azure Custom Vision service を使用して独自のイメージ分類モデルを構築し、既存のモデルを他のフレームワークから ONNX に <a href=\"https://docs.microsoft.com/en-us/windows/ai/convert-model-winmltools\" target=\"_blank\">変換</a> する、または <a href=\"https://github.com/Azure/MachineLearningNotebooks/tree/master/training\" target=\"_blank\">AzureML でカスタムモデルをトレーニング</a> して、ONNX 形式で保存することができます。</p>\n\n<p><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/bd82ae3d-c003-4ffa-be31-213ab02b2c4f.png\"><img alt=\"4Ways\" border=\"0\" height=\"616\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/a4b73ece-3c76-4e4a-a867-d6a8314fdc82.png\" style=\"margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;\" title=\"4Ways\" width=\"821\"></a></p>\n\n<h2>ONNX Runtime を使用した推論</h2>\n\n<p>ONNX 形式でトレーニング済みのモデルを作成した後は&#39;、推論の ONNX Runtime を使用してフィードする準備ができています。 構築済みの Python パッケージには、さまざまな実行プロバイダーとの統合が含まれており、コンピューティングの待機時間が少なく、リソース使用率が高くなっています。 GPU のビルドには、CUDA 9.1 が必要です。</p>\n\n<p>開始するには、PyPi から必要なパッケージを Python 環境にインストールします。</p>\n\n<pre>\npip install onnxruntime \npip install onnxruntime-gpu</pre>\n\n<p>次に、推論セッションを作成して、モデルの操作を開始します。</p>\n\n<pre>\nimport onnxruntime\nsession = onnxruntime.InferenceSession(&quot;your_model.onnx&quot;)  </pre>\n\n<p>最後に、選択した出力と入力を使用して推論セッションを実行し、予測された値を取得します。</p>\n\n<pre>\nprediction = session.run(None, {&quot;input1&quot;: value})</pre>\n\n<p>詳細については、 <a href=\"https://aka.ms/onnxruntime-python\" target=\"_blank\">API に関する完全なドキュメント</a>を参照してください。  </p>\n\n<p>これで、使用するアプリケーションまたはサービスの <a href=\"https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-build-deploy-onnx\" target=\"_blank\">ONNX モデルをデプロイ</a> する準備ができました。</p>\n\n<h2>今すぐ始めましょう</h2>\n\n<p>オープンで相互運用可能な AI のエキスパートとして、新たに魅力的な AI イノベーションを効率的に提供するための製品とツールの構築に積極的に投資しています。 コミュニティが参加して ONNX Runtime を試すことをお待ちしています。 <a href=\"https://pypi.org/project/onnxruntime\" target=\"_blank\">ONNX Runtime をインストール</a>してすぐに始めて、 <a href=\"https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureMachineLearningService\" target=\"_blank\">Azure Machine Learning サービスフォーラム</a>にフィードバックをお寄せください。</p>"
