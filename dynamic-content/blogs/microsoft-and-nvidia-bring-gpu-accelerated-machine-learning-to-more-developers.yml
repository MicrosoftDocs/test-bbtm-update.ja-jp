### YamlMime:Yaml
ms.openlocfilehash: c2039c1d7aec90fbb8d2f7e9d01b879726ca16cf
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/11/2022
ms.locfileid: "139909686"
Slug: microsoft-and-nvidia-bring-gpu-accelerated-machine-learning-to-more-developers
Title: Microsoft と NVIDIA は、GPU アクセラレータを使用した機械学習を、より多くの開発者に提供します
Summary: データ量と待機時間の要件が増え続け、GPU は機械学習 (ML) を大規模に実行する重要なツールになりました。 この週は、Microsoft と NVIDIA が組み合わせて構築した 2 つの統合を発表し、より多くの開発者やデータ サイエンティスト向けの業界をリードする GPU アクセラレーションのロックを解除します。
Content: >-
  <p>データ量と待機時間の要件が増え続け、GPU は機械学習 (ML) を大規模に実行する重要なツールになりました。 この週は、Microsoft と NVIDIA が組み合わせて構築した 2 つの統合を発表し、より多くの開発者やデータ サイエンティスト向けの業界をリードする GPU アクセラレーションのロックを解除します。</p>


  <ul>
      <li><a href="https://azure.microsoft.com/en-us/services/machine-learning-service/">Azure Machine Learning サービス</a>は、従来の機械学習の実践者が NVIDIA GPU を使用してパイプラインを簡単に加速できる、NVIDIA のオープン ソース ソフトウェア ライブラリである RAPIDS を統合する最初の主要なクラウド ML サービスです</li>
      <li><a href="https://azure.microsoft.com/en-us/blog/onnx-runtime-is-now-open-source/">ONNX Runtime は</a> 、NVIDIA TensorRT アクセラレーション ライブラリを統合しました。ディープ ラーニングの実践者は、フレームワークの選択に関係なく、非常に高速なインフェレンシングを実現できます。</li>
  </ul>


  <p>これらの統合は、Azure 上の NVIDIA GPU テクノロジが既に豊富に組み込MLされています。</p>


  <p><em>&ldquo;NVIDIA と Microsoft&rdquo; は、フレームワークの選択に関係なく、開発者やデータ サイエンティスト向けのエンドツーエンドのデータ サイエンス パイプラインの高速化に取り組み、NVIDIA &ldquo; の高速コンピューティング ソフトウェアの製品管理のシニア ディレクターである Briski 氏は述べています。NVIDIA TensorRT と ONNX Runtime および RAPIDS を Azure Machine Learning&rsquo; サービスと統合することで、機械学習の実践者がデータ サイエンス ワークフロー全体で NVIDIA GPU を活用しやすくなっています。&rdquo;</em></p>


  <h2>Azure Machine Learningサービスと NVIDIA RAPIDS の統合</h2>


  <p>Azure Machine Learningサービスは、RAPIDS を統合する最初の主要なクラウド ML サービスで、従来の機械学習パイプラインに最大 20 倍の高速化を提供します。 RAPIDS は、GPU 高速機械学習を実行するための NVIDIA CUDA 上に構築されたライブラリのスイートであり、データ準備とモデルトレーニングの高速化を可能にしています。 RAPIDS は、NVIDIA GPU の機能を活用することで、一般的なデータ サイエンス タスクを大幅に加速します。</p>


  <p>シンプルな <a href="https://github.com/Azure/MachineLearningNotebooks/blob/master/contrib/RAPIDS/azure-ml-with-nvidia-rapids.ipynb">Jupyter Notebook</a> として Azure Machine Learning サービスで公開されている RAPIDS では、高パフォーマンスの GPU 実行に NVIDIA CUDA が使用され、使い方の良い Python インターフェイスを介して GPU の並列処理と高メモリ帯域幅が公開されます。 これには、Pandas ユーザーになじみのある cuDF と呼ばれるデータフレーム ライブラリと、Scikit-learn で使用できるすべての機械学習アルゴリズムの GPU バージョンを提供する cuML と呼ばれる ML ライブラリが含まれています。 また、DASK を使用すると、RAPIDS は Azure 上のマルチノードのマルチ GPU 構成を利用できます。</p>


  <p><a href="https://aka.ms/rapids-blog">サービスの RAPIDS</a> の詳細Azure Machine Learning NVIDIA GTC で <a href="https://gputechconf2019.smarteventscloud.com/connect/search.ww#loadSearch-searchPhrase=microsoft&amp;searchType=session&amp;tc=0&amp;sortBy=dayTime&amp;p=">Azure セッションの RAPIDS</a> に参加してください。</p>


  <h2>プレビューでの ONNX Runtime と NVIDIA TensorRT の統合</h2>


  <p>ONNX Runtime の NVIDIA TensorRT 実行プロバイダーのプレビューをオープン ソースに公開できます。 このリリースでは、開発者が選択したフレームワークに関係なく、業界をリードする GPU アクセラレーションを簡単に活用することで、オープンで相互運用可能な AI に向けた別の一歩を踏み出しています。 開発者は、ONNX Runtime を介して TensorRT の機能を利用して <a href="https://onnx.ai/">、PyTorch</a> 、TensorFlow、MXNet などの多くの一般的なフレームワークからエクスポートまたは変換できる ONNX モデルのインフェレンシングを加速できます。 現在、ONNX Runtime は、数十億人のユーザーにサービスを提供する主要なシナリオを、Bing、Officeに提供しています。</p>


  <p>TensorRT 実行プロバイダーでは、ONNX ランタイムは、汎用 GPU アクセラレーションと比較して、同じハードウェアで優れたインフェレンシング パフォーマンスを提供します。 MultiMedia サービスの内部ワークロードで TensorRT 実行プロバイダーを使用して、最大 2 倍のパフォーマンスBing確認しました。</p>


  <p>詳細については、ONNX Runtime と <a href="https://aka.ms/trt-blog"></a> TensorRT の統合に関する詳細なブログを参照するか、NVIDIA GTC の <a href="https://gputechconf2019.smarteventscloud.com/connect/search.ww#loadSearch-searchPhrase=ONNX&amp;searchType=session&amp;tc=0&amp;sortBy=dayTime&amp;p=">ONNX</a> セッションに参加してください。</p>


  <h2>すべての人に対する機械学習の高速化</h2>


  <p>NVIDIA とのコラボレーションは、開発者やデータ サイエンティストがイノベーションをより迅速に実現するのに役立つもう 1 つのマイルストーンです。 私たちは、フレームワーク、ツール、アプリケーションの選択に関係なく、すべての機械学習の実践者の生産性の向上に取り組みを行っています。 これらの新しい統合により、AI のイノベーションを促進し、コミュニティに試用を強く促すのが容易になります。フィードバックをお待ちしております。</p>
