### YamlMime:Yaml
ms.openlocfilehash: ee04aa03856a2ef90627d32ccbf82d3298a05428
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/11/2022
ms.locfileid: "139891330"
Slug: advancing-microsoft-teams-on-azure-operating-at-pandemic-scale
Title: Azure で Microsoft Teams を進める— pandemic scale で運用
Summary: スケール、回復性、およびパフォーマンスは夜間に発生しません。これは、ユーザーを満足させるする製品を構築するための、継続的な投資や、日々の投資や、パフォーマンスを第一に考えています。
Content: >-
  <p>&ldquo;COVID-19 pandemic は、作業、調査、および socialize の意味をリセットしました。 多くの人と同様に、私は同僚への接続として Microsoft Teams を利用しています。 この投稿では、Microsoft Teams 製品グループ &mdash; <strong>pa-rish tandon</strong> (Corporate バイスプレジデント)、 <strong>Aarthi Natarajan</strong> (グループエンジニアリングマネージャー)、および<strong>Martin Taillefer</strong> (設計者) &mdash; の友人が、エンタープライズレベルのセキュリティで保護された生産性アプリの管理とスケーリングについて、いくつかの学習を共有しています。 &rdquo; -Mark Russinovich, CTO, Azure</p>


  <hr>

  <p>&nbsp;</p>


  <p>スケール、回復性、およびパフォーマンスは、継続的な投資や、1日に何日もかかることはありません。また、ユーザーに満足させる製品を構築するためのパフォーマンスを第一に考えてい &mdash; ます。 起動以来、Teams は2017年7月に2019年7月のユーザーから1300万年11月2019の2000万まで、大幅な成長を経験してきました。 4月に、1日あたり7500万を超えるアクティブユーザー、2億の日単位の会議の参加者、および41億の毎日の会議の分数がある Teams を共有しました。 このようなペースでサービスを拡張するために必要な継続的な作業に慣れてきたと思いましたが、これまでの急速な成長に Teams ています。 COVID-19 は、この前提に直面しています。この経験により、以前は unthinkable の成長期間にサービスを実行し続けることができますか?</p>


  <h2>堅固な基盤</h2>


  <p>Teams は、マイクロサービスアーキテクチャに基づいて構築されており、数百のマイクロサービスが動作し、メッセージング、会議、ファイル、カレンダー、アプリなどの多くの機能を提供 &rsquo; しています。 マイクロサービスを使用すると、各コンポーネントチームが個別に変更を行うことができます。</p>


  <p>Azure は、すべての Microsoft &rsquo; クラウドサービス (Microsoft Teams を含む) を過小にピン留めするクラウドプラットフォームです。 Microsoft のワークロードは Azure の仮想マシン (Vm) で実行されます。以前のサービスは azure <a href="https://azure.microsoft.com/en-us/services/cloud-services/" target="_blank">Cloud Services</a>でデプロイされ、 <a href="https://azure.microsoft.com/en-us/services/service-fabric/" target="_blank">azure Service Fabric</a>には新しいサービスがデプロイされています。 主なストレージスタックは<a href="https://azure.microsoft.com/en-us/services/cosmos-db/" target="_blank">Azure Cosmos DB</a>、一部のサービスでは<a href="https://azure.microsoft.com/en-us/services/storage/blobs/" target="_blank">Azure Blob Storage</a>を使用します。 スループットと回復性を向上させるために、 <a href="https://azure.microsoft.com/en-us/services/cache/" target="_blank">Redis の Azure cache</a> をカウントします。 <a href="https://azure.microsoft.com/en-us/services/traffic-manager/" target="_blank">Traffic Manager</a>と Azure の<a href="https://azure.microsoft.com/en-us/services/frontdoor/" target="_blank">フロントドア</a>を活用して、必要な場所にトラフィックをルーティングします。 <a href="https://azure.microsoft.com/en-us/services/storage/queues/" target="_blank">Queue Storage</a>と<a href="https://azure.microsoft.com/en-us/services/event-hubs/" target="_blank">Event Hubs</a>を使用して通信を行い、テナントとユーザーを管理する<a href="https://azure.microsoft.com/en-us/services/active-directory/" target="_blank">Azure Active Directory</a>に依存しています。</p>


  <p>&nbsp;</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/18c7ac34-1b68-4073-b549-945fdda49e0e.png"><img alt="    Diagram showing that Azure is the platform that underpins Teams Services and Office 365 Core Service" border="0" height="426" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/c229cba7-597c-46dc-8545-7f42706d8c55.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="" width="640"></a></p>


  <p>&nbsp;</p>


  <p>この記事は主にクラウドバックエンドに焦点を当てていますが、 &rsquo; Teams クライアントアプリケーションでも最新の設計パターンとフレームワークを使用し、充実したユーザーエクスペリエンスを提供し、オフラインまたは断続的に接続されたエクスペリエンスをサポートすることに注目してください。 クライアントを迅速かつサービスと並行して更新するための主要な機能は、迅速な反復処理のための重要な機能です。 &rsquo;アーキテクチャをさらに掘り下げたい場合は、 <a href="https://myignite.techcommunity.microsoft.com/sessions/83471" target="_blank">Microsoft Ignite 2019</a>からこのセッションを確認してください。</p>


  <h2>アジャイル開発</h2>


  <p>Microsoft の CI/CD パイプラインは<a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops" target="_blank">Azure Pipelines</a>上に構築されています。 自動化されたエンドツーエンドテストとテレメトリシグナルの組み合わせに基づいて、ゲートに基づくリングベースの展開戦略を使用します。 Microsoft のテレメトリ信号はインシデント管理パイプラインと統合され、サービスとクライアント定義の両方のメトリックに関するアラートを提供します。 Microsoft では、分析のために <a href="https://azure.microsoft.com/en-us/services/data-explorer/" target="_blank">Azure データエクスプローラー</a> に大きく依存しています。</p>


  <p>さらに、クラッシュ率、メモリ消費、アプリケーションの応答性、パフォーマンス、ユーザーエンゲージメントなどの主要な製品メトリックに対する機能の動作を評価するスコアカードを含む実験パイプラインを使用します。 これは、新しい機能が希望どおりに動作しているかどうかを判断するのに役立ちます。</p>


  <p>すべてのサービスとクライアントは、集中管理された構成管理サービスを使用します。 このサービスは、製品の機能のオン/オフの切り替え、キャッシュの有効期限の値の調整、ネットワーク要求の頻度の制御、Api に接続するためのネットワークエンドポイントの設定を行うための構成状態を提供します。 これにより、darkly &rdquo; を起動する &ldquo; 柔軟なフレームワークが提供されます。また、変更の影響を正確に測定して、すべてのユーザーに対して安全で効率的であることを確認することができます。</p>


  <h2>主要回復性戦略</h2>


  <p>マイクロソフトでは、さまざまなサービスに対していくつかの回復性戦略を採用しています。</p>


  <ul>
      <li>アクティブ<strong>/アクティブフォールトトレラント</strong>システム: アクティブ/アクティブフォールトトレラントシステムは、運用に依存しない2つ以上の異種パスとして定義されています。各パスは、安定した状態でライブトラフィックを処理するだけでなく、予想されるトラフィックの100% を提供しながら、シームレスなフェールオーバーのためにクライアントとプロトコルのパス選択を この戦略は、異種システムの構築と保守を正当化するために、非常に大きな障害ドメインや顧客への影響を妥当なコストで実現する場合に採用しています。 たとえば、外部から参照できるすべてのクライアントドメインに対して Office 365 DNS システムを使用します。 さらに、静的な CDN クラスのデータは、Azure のフロントドアと akamai の両方でホストされます。</li>
      <li><strong>回復性に最適化</strong>されたキャッシュ: パフォーマンスと回復性の両方において、コンポーネント間のキャッシュを広範囲にわたって活用します。 キャッシュは、ダウンストリームサービスが使用できなくなった場合に、平均待機時間を短縮し、データのソースを提供するのに役立ちます。 データを長期間キャッシュに保持すると、データの鮮度の問題が発生しますが、ダウンストリームの障害に対しては最適な防御策となります。 キャッシュデータと Time to Live (TTL) を更新する時間に焦点を当てます。 長い TTL と短い TTR 値を設定することにより、データを維持するための最新の方法と、ダウンストリームの依存関係が失敗したときにデータをどの程度保持するかを微調整できます。</li>
      <li><a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker" target="_blank"><strong>サーキットブレーカー</strong></a>: これは、サービスが失敗する可能性のある操作を実行できないようにするための一般的な設計パターンです。 これにより、再試行要求によって過負荷になることなくダウンストリームサービスを復旧できるようになります。 また、依存関係に問題が発生した場合のサービスの応答も改善され、システムがエラー条件を許容できるようになります。</li>
      <li><a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead" target="_blank"><strong>バルクヘッド分離</strong></a>: 重要なサービスの一部を完全に分離されたデプロイに分割します。 1つのデプロイで問題が発生した場合、バルクヘッド分離は、他の展開の運用を支援するように設計されています。 この軽減策により、できるだけ多くの顧客の機能が保持されます。</li>
      <li><a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/throttling" target="_blank"><strong>Api レベルの制限</strong></a>: 重要なサービスが api レベルで要求を調整できることを確認します。 これらのレート制限は、上で説明した一元化された構成管理システムを使用して管理されます。 この機能により、COVID-19 のサージ発生時に重要ではない Api を制限するようになりました。</li>
      <li><a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/retry" target="_blank"><strong>効率的な再試行パターン</strong></a>: すべての API クライアントが効率的な再試行ロジックを実装することを保証し、検証します。これにより、ネットワーク障害が発生した場合にトラフィックストームが回避されます。</li>
      <li><strong>タイムアウト</strong>: タイムアウトセマンティクスを一貫して使用することで、下流の依存関係で何らかの問題が発生したときに作業が停止するのを防ぐことができます。</li>
      <li><strong>ネットワーク障害の正常な処理</strong>: オフラインまたは接続が不十分なときにクライアントエクスペリエンスを向上させるために、長期的な投資が行われています。 この分野の主な改善点は、COVID 19 のサージが開始されたときと同じように運用環境に立ち、クライアントがネットワークの品質に関係なく一貫したエクスペリエンスを提供できるようにすることです。</li>
  </ul>


  <p>Azure <a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/" target="_blank">クラウドの設計パターン</a>を確認した経験があれば、これらの概念の多くはお客様にとってなじみがあるかもしれません。 &nbsp; また、これらのパターンのいくつかの実装を提供するマイクロサービスでは、この <a href="https://github.com/App-vNext/Polly" target="_blank">ライブラリ</a> を多用しています。</p>


  <p>私たちのアーキテクチャは Teams、私たちにとっては私たちにとっても、私たちのアーキテクチャは月を超えています。また、需要に合わせて簡単にスケーリングできます。 ただし、スケーラビリティは設定されて &rdquo; いない &ldquo; ので注意が必要です。複雑なシステムで差し迫っ動作に対処するには、継続的に注意する必要があります。</p>


  <p><strong>自宅での最新の注文が世界中に開始されるようになったときには、システムに組み込まれているアーキテクチャの柔軟性を活用し、すべてのノブを有効にして、急速に増加する需要に効果的に対応する必要がありました。</strong></p>


  <h2>容量の予測</h2>


  <p>どのような製品でも、未加工のユーザーと使用パターンの両方において、増加が生じる場所を予測するために、モデルを継続的に反復処理します。 モデルは、履歴データ、周期的なパターン、新しい着信する大規模な顧客、およびその他のさまざまなシグナルに基づいています。</p>


  <p>サージが開始されたとき、以前の予測モデルがすぐに廃止されたことが明らかになったので、グローバルな需要の増加を考慮に入れて新たに作成する必要がありました。 既存のユーザーからの新しい使用パターンが表示されていました。また、既存のが休止していないユーザーからの新しい使用状況と、多数の新規ユーザーが同時に製品にオンボードしています。 さらに、潜在的なコンピューティングおよびネットワークのボトルネックに対処するために、迅速にリソース意思決定を行う必要がありました。 複数の予測モデリング手法 (<a href="https://otexts.com/fpp2/arima.html" target="_blank">ARIMA</a>、加法、乗法、対数) を使用します。 これまで、予測が過剰にならないように、国ごとの基本キャップを追加しました。 モデルは、業界および地域ごとの使用量によって、大規模な変数と成長パターンを把握しようとしています。 <a href="https://coronavirus.jhu.edu/" target="_blank">ジョンズホプキンス &rsquo; </a> research を含む外部データソースを国別に統合して、ボトルネック領域のピーク時の負荷予測を強化しています。</p>


  <p>このプロセス全体を通して、私たちは誤りに注意を払っ &mdash; ています。また、使用パターンが安定しているため、必要に応じてスケールバックしました。</p>


  <h2>コンピューティングリソースのスケーリング</h2>


  <p>一般に、自然災害に耐えられるように Teams を設計します。 複数の Azure リージョンを使用することにより、データセンターの問題だけでなく、中断から主要地域までのリスクを軽減することができます。 ただし、これは、このような不測中に影響を受けるリージョン &rsquo; の負荷に対応するために、追加のリソースをプロビジョニングすることを意味します。 スケールアウトするために、すべての重要なマイクロサービスのデプロイを、すべての主要な Azure geography の追加リージョンに迅速に拡張しています。 地理的なリージョンの合計数を増やすことで、緊急時の負荷を吸収するために各リージョンに保持する必要がある予備容量の総量が減少し、合計容量のニーズが軽減されます。 この新しいスケールで負荷を処理することで、効率を向上させる方法について、いくつかの洞察が得られるようになりました。</p>


  <ul>
      <li>いくつかのマイクロサービスを再デプロイすることで、より多くの小規模なコンピューティングクラスターを優先することで、クラスターごとのスケーリングに関する考慮事項を回避し、デプロイを高速化し、さらに細かい負荷分散を行うことができることがわかりました。</li>
      <li>以前は、さまざまなマイクロサービスに使用されている特定の仮想マシン (VM) の種類に依存していました。 VM の種類または CPU に関して柔軟性が高く、コンピューティング能力やメモリ全体を重視することで、各リージョンで Azure リソースをより効率的に使用できるようになりました。</li>
      <li>サービスコード自体の最適化の機会が見つかりました。 たとえば、いくつかの単純な改善によって、アバターの生成に費やされる CPU 時間が大幅に短縮されました (ユーザーの画像が使用できないときに使用される、これらの小さなバブルを使用します)。</li>
  </ul>


  <h2>ネットワークとルーティングの最適化</h2>


  <p>ほとんどの Teams &rsquo; 容量の消費量は、特定の Azure geography では夜間に発生し、アイドル状態のリソースになります。 このアイドル容量を活用するためのルーティング戦略を実装しました (常にコンプライアンスとデータの保存場所の要件を尊重しています)。</p>


  <ul>
      <li>非対話型のバックグラウンド作業は、現在のアイドル状態に動的に移行されます。 これは、トラフィックが適切な場所に配置されるように、Azure フロントドアの API 固有のルートをプログラミングすることによって行われます。</li>
      <li>トラフィックの呼び出しとミーティングは、複数のリージョンにわたってルーティングされ、サージを処理しました。 負荷を効果的に分散するために Azure Traffic Manager を使用し、観察された使用パターンを活用しています。 また、ワイドエリアネットワーク (WAN) の調整を防ぐために、時間単位の負荷分散を行う runbook の作成に取り組んでいます。</li>
  </ul>


  <p>一部の Teams &rsquo; クライアントトラフィックは、Azure のフロントドアで終了します。 ただし、より多くのリージョンでより多くのクラスターをデプロイしたため、新しいクラスターは十分なトラフィックを得られませんでした。 これは、ユーザーの場所と Azure のフロントドアノードの場所の分布の成果物でした。 この不均一なトラフィック分散に対処するために、Azure Front ドア &rsquo; を使用して、国レベルでトラフィックをルーティングしました。 この例では、1つのサービスの英国西部リージョンにさらにフランスのトラフィックをルーティングした後に、トラフィックの分散を向上させることができます。</p>


  <p align="center"><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/8125fe92-6b30-4d30-846c-fb22fa82d630.png"><img alt=" Graph showing improved traffic distribution after routing additional France traffic to the UK West region" border="0" height="284" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/588d8f8c-4b7d-4e63-8af5-2989021fbcab.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="" width="1024"></a><em>&nbsp;<br>

  図 1: リージョン間でトラフィックをルーティングした後のトラフィック分布の向上</em></p>


  <h2>キャッシュとストレージの機能強化</h2>


  <p>多数の分散キャッシュを使用しています。 多数のビッグ分散キャッシュ。 トラフィックが増加するにしたがって、個々のキャッシュがスケーリングされないポイントへのキャッシュの負荷も増加しました。 キャッシュの使用に大きな影響を与える単純な変更をいくつかデプロイしました。</p>


  <ul>
      <li>生の JSON ではなくバイナリ形式でキャッシュ状態を格納し始めました。 これにはプロトコル バッファー形式を使用しました。</li>
      <li>キャッシュに送信する前に、データの圧縮を開始しました。 優れた速度と圧縮率により、LZ4 圧縮を使用しました。</li>
  </ul>


  <p>ペイロード サイズの 65% の削減、逆シリアル化時間の 40% の削減、シリアル化時間の 20% の削減を実現しました。 すべての結果を得る。</p>


  <p>調査の結果、キャッシュのいくつかに過剰に積極的な TTL 設定が含まれており、不要な一意のデータ削除が発生する可能性が明らかになりました。 これらの TTL を増やすと、ダウンストリーム システムの平均待機時間と負荷の両方を削減するのに役立ちます。</p>


  <h2>目的に合った低下 (特徴のブラウンアウト)</h2>


  <p>&rsquo;&rsquo;私たちは、何をプッシュする必要があるのか本当にわからなかったので、追加の Teams 容量をオンラインにするための時間を購入するために、予期しない需要の急増に迅速に対応できるメカニズムを配置することを慎重に決定しました。</p>


  <p>すべての機能が、お客様と同等の重要性を持つわけではありません。 たとえば、メッセージの送受信は、他のユーザーが現在メッセージを入力しているのを確認する機能よりも重要です。 このため、サービスのスケールアップに取り組む間、2 週間の間、入力インジケーターをオフにしました。 これにより、ピークトラフィックがインフラストラクチャの一部に対して 30% 減少しました。</p>


  <p>通常、必要なデータが近い場所に近い、エンドツーエンドの平均待機時間を短縮する、アーキテクチャの多くのレイヤーで積極的なプリフェッチを使用します。 ただし、プリフェッチはコストがかかる可能性があります。そのため、使用しないデータをフェッチするときに無駄な作業が発生し、プリフェッチされたデータを保持するためにストレージ リソースが必要になります。 一部のシナリオでは、プリフェッチを無効にし、一部のサービスの容量を解放して待機時間を長くすることを選択しました。 それ以外の場合は、プリフェッチ同期間隔の期間を長くしました。 そのような例の 1 つは、要求ボリュームを 80% 削減したモバイルでのカレンダープリフェッチを抑制することでした。<br>

  &nbsp;</p>


  <p align="center"><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/6529abae-ee4b-469a-80f8-cb8d7c8c224d.png"><img alt=" Graph showing that suppressing calendar prefetch on mobile reduced request volume by 80%" border="0" height="589" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/4f02c6cf-a077-4d97-9620-1b410bbfdd1e.png" style="border: 0px currentcolor; border-image: none; display: inline; background-image: none;" title="" width="970"></a><br>

  <em>図 2: モバイルでカレンダー イベントの詳細のプリフェッチを無効にする。</em></p>


  <h2>インシデント管理</h2>


  <p>成熟したインシデント管理プロセスを使用してシステムの正常性を追跡および維持しますが、このエクスペリエンスは異なっています。 トラフィックの急増に対処しただけでなく、エンジニアや同僚は、自宅での作業に適応しながら、個人的および感情的な課題を経験しました。</p>


  <p>お客様だけでなくエンジニアもサポートするために、いくつかの変更を行いました。</p>


  <ul>
      <li>インシデント管理のローテーションを毎週の実行時間から 1 日のデータに切り替えました。</li>
      <li>すべてのオンコール エンジニアは、シフト間に少なくとも 12 時間オフしました。</li>
      <li>会社全体からより多くのインシデント マネージャーを取り込み、</li>
      <li>サービス全体で重要でない変更はすべて延期しました。</li>
  </ul>


  <p>これらの変更は、すべてのインシデント マネージャーとオンコール エンジニアが、お客様の要求を満たしながら、自宅のニーズに集中するのに十分な時間を確保するのに役立ちます。</p>


  <h2>将来のTeams</h2>


  <p>数年前に起こった場合、この状況はどうだったのか、振り返って考えるのは魅力的です。 クラウド コンピューティングを使用せずに行ったようなスケーリングは不可能でした。 単に構成ファイルを変更するだけで、現在実行できる操作には、以前は新しい機器や新しい建物を購入する必要が生じていました。 現在のスケーリングの状況が安定する中で、私たちは将来への注意を返しています。 インフラストラクチャを改善する機会は多数あると考えています。</p>


  <ul>
      <li>Azure Kubernetes Service を使用して VM ベースのデプロイからコンテナー ベースのデプロイに移行する予定です。これは、運用コストを削減し、機敏性を向上し、業界に合わせて調整する予定です。</li>
      <li>REST の使用を最小限に抑え、gRPC などのより効率的なバイナリ プロトコルを使用する必要があります。 システム全体のポーリングの複数のインスタンスを、より効率的なイベント ベースのモデルに置き換える予定です。</li>
      <li>システムの信頼性を高め、常に完全に機能し、行動を起こす準備ができていることを保証するために、混乱エンジニアリングプラクティスを体系的に採用しています。</li>
  </ul>


  <p>アーキテクチャを業界のアプローチに合わせて維持し、Azure チームのベスト プラクティスを活用することで、支援を求め必要なときに、専門家は、データ分析、監視、パフォーマンスの最適化、インシデント管理などの問題を迅速に解決するのに役立ちます。 Microsoft は、Microsoft 全体の同僚と、より広範なソフトウェア開発コミュニティの開放性に感謝しています。 アーキテクチャとテクノロジは重要ですが、システムを正常に保つのは、ユーザーのチームです。</p>


  <p>&nbsp;</p>


  <hr>

  <p><em>関連する投稿: </em><a href="https://aka.ms/AdvancingReliability/6" target="_blank"><em>Azure は COVID-19 に応答します</em></a><em>。<br>

  関連記事: </em> COVID-19 パンデミック中の Microsoft、顧客を支援する <a href="https://aka.ms/AA8ad9b" target="_blank"><em>Azures&rsquo; 容量の拡大</em></a><em>。</em></p>
