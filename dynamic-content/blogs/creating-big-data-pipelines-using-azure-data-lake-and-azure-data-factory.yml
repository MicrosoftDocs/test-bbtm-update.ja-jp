### YamlMime:Yaml
ms.openlocfilehash: 074e33c057e3a8b8ebc1189d5cf6c18e329924fb
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/11/2022
ms.locfileid: "139910836"
Slug: creating-big-data-pipelines-using-azure-data-lake-and-azure-data-factory
Title: Azure Data Lake と Azure Data Factory を使用したビッグデータパイプラインの作成
Summary: 今週、Microsoft では、ビッグデータの処理と分析をよりシンプルで使いやすくするために、新規および拡張された Azure Data Lake のパブリックプレビューを発表しました。
Content: "<p>今週、Microsoft では、ビッグデータの処理と分析をよりシンプルで使いやすくするために、新規および拡張された Azure Data Lake のパブリックプレビューを <a href=\"https://azure.microsoft.com/en-us/blog/azure-data-lake-reaches-public-preview/\">発表</a> しました。 展開された Azure Data Lake には、Azure Data Lake Store、Azure Data Lake Analytics、Azure HDInsight が含まれています。</p>\n\n<p>Azure Data Lake Store には、データスケールとしてアプリケーションに変更を加えることなく、任意のサイズ、種類、速度のデータを簡単にキャプチャできる単一のリポジトリが用意されています。 Azure Data Lake Analytics は、Apache YARN &nbsp; 上に構築された新しいサービスであり、SQL の利点とユーザーコードの表現力を統一した言語 U-SQL を含みます。 このサービスは動的に規模を調整し、Azure Active Directory を通じてエンタープライズレベルのセキュリティを使用してあらゆる種類のデータに対して分析を行うことができるため、ビジネス目標に専念できます。</p>\n\n<p>10月の第1週には、Azure HDInsight の既存のサポートを使用することに加えて、Azure Data Lake と Azure Data Factory を使用してビッグデータパイプライン (ワークフロー) を作成および運用化できることが <a href=\"https://azure.microsoft.com/en-us/blog/create-big-data-pipelines-using-azure-data-lake-store-analytics-azure-data-factory/\">発表</a> されました。 現在、これらの新しく追加された機能の <strong>パブリックプレビュー</strong> が発表されています。 Azure Data Lake と Azure Data Factory の統合により、次のことが可能になります。</p>\n\n<h2><font size=\"5\">Azure Data Lake Store にデータを簡単に移動する</font></h2>\n\n<p>現時点では、Azure Data Factory は、次のソースから Azure Data Lake Store へのデータの移動をサポートしています。</p>\n\n<ul>\n <li>Azure BLOB</li>\n <li>Azure SQL データベース</li>\n <li>Azure テーブル</li>\n <li>オンプレミスの SQL Server データベース</li>\n <li>Azure DocumentDB</li>\n <li>Azure SQL DW</li>\n <li>オンプレミスのファイル システム</li>\n <li>オンプレミスの Oracle データベース</li>\n <li>オンプレミスの MYSQL データベース</li>\n <li>オンプレミスの DB2 データベース</li>\n <li>オンプレミスの Teradata データベース</li>\n <li>オンプレミスの Sybase データベース</li>\n <li>オンプレミスの PostgreSQL データベース</li>\n <li>オンプレミスの HDFS</li>\n <li>汎用 OData (近日公開予定)</li>\n <li>汎用 ODBC (近日公開予定)</li>\n</ul>\n\n<p>また、Azure Data Lake Store から Azure Blob、Azure SQL Database、オンプレミスのファイルシステムなどのさまざまなシンクにデータを移動することもできます。Azure Blob Storage から Azure Data Lake Store にデータを移動するには、次の手順に従います。</p>\n\n<p>注: 次の手順に従う前に、有効な Azure Data Lake Store アカウントを持っている必要があります。 お持ち <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-lake-store-get-started-portal/\">で</a> ない場合 &rsquo; は、ここをクリックして新しいアカウントを作成します。</p>\n\n<h3>Azure データ ファクトリを作成する</h3>\n\n<p>Azure Portal にログインし、Azure Data Factory に移動します。 名前を入力し、[サブスクリプション]、[リソースグループ名]、[リージョン名] を選択します。 &rsquo;AzureDataLakeStoreAnalyticsSample という名前を指定します。</p>\n\n<p><img alt=\"2015-10-25_22h22_06\" border=\"0\" height=\"869\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/fa29f0fd-5f04-492b-864f-d760326da94c.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_22h22_06\" width=\"1168\"></p>\n\n<p>作成したら、データファクトリに移動し、[作成およびデプロイ] をクリックします。</p>\n\n<p><img alt=\"2015-10-25_22h37_40\" border=\"0\" height=\"663\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/9f970234-e70b-4336-af90-f8d797ec2315.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_22h37_40\" width=\"581\">&nbsp;</p>\n\n<h3>ADF のリンクされたサービスを作成する</h3>\n\n<p>Azure Storage のリンクされたサービスを作成する: これは、データの移動元の Azure Blob Storage (ソース) です。</p>\n\n<p>[新しいデータストア &ndash; &gt; Azure Storage] をクリックします。 AccountName &gt; および &lt; AccountKey &gt; パラメーターの &lt; 値を入力し、[デプロイ] をクリックします。</p>\n\n<p><img alt=\"2015-10-27_14h55_19\" border=\"0\" height=\"434\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/aba7e2d0-e2ed-45b8-88fb-39c069c9422b.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_14h55_19\" width=\"316\"></p>\n\n<p>Azure Data Lake Store のリンクされたサービスの作成: データを移動する Azure Data Lake Storage (シンクの別名) です。</p>\n\n<p>[新しいデータストア-Azure Data Lake Store] を &gt; クリックします。</p>\n\n<p><img alt=\"2015-10-25_23h11_00\" border=\"0\" height=\"439\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/25ec0c3b-5f9b-42ae-b701-ea11b04d5461.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_23h11_00\" width=\"319\"></p>\n\n<p>Azure Data Lake Store のリンクされたサービスに必須のパラメーターを入力してください</p>\n\n<p>DataLakeUri: 上記の手順で作成するか、既存のものを使用します。 たとえば、 <a href=\"https://&lt;adlstoreaccountname&gt;.azuredatalakestore.net/webhdfs/v1\">https:// &lt; adlstoreaccountname &gt; . azuredatalakestore.net/webhdfs/v1</a>のようになります。 Adlstoreaccountname &gt; を ADL ストアのアカウント名に置き換え &lt; ます。</p>\n\n<p>承認: このパラメーターを入力するには、[承認] をクリックします。 ポップアップが表示され、資格情報を入力する必要があります。</p>\n\n<p><img alt=\"2015-10-25_23h26_03\" border=\"0\" height=\"387\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/fedcaffb-9040-402a-aaef-e7abe79d8979.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_23h26_03\" width=\"380\"></p>\n\n<p>Azure Data Lake Store アカウントが別のサブスクリプションにあり、データファクトリとは異なるリソースグループ名の下にある場合は、次のパラメーターも入力する必要があります。</p>\n\n<ul>\n <li>AccountName</li>\n <li>SubscriptionID</li>\n <li>ResourceGroupName</li>\n</ul>\n\n<p>[デプロイ] をクリックします。 これにより、Azure Data Lake Store のリンクされたサービスが作成されます。</p>\n\n<p>注: デプロイを開始する前に値を指定していない場合は、Json で省略可能な行を削除する必要があります。</p>\n\n<h3>ADF データセットの作成</h3>\n\n<p>Azure Blob Storage ソースデータセットを作成します。</p>\n\n<p>[新しいデータセット &ndash; &gt; ] [Azure Blob ストレージ] をクリックします。</p>\n\n<p><img alt=\"2015-10-26_07h58_55\" border=\"0\" height=\"437\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/310438c0-30bd-44c7-8aac-630836f2a0cc.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_07h58_55\" width=\"315\"></p>\n\n<p>これにより、任意の値を入力できる Azure Blob ストレージデータセットのテンプレートが表示されます。 次の例に示すように、Azure Blob storage のデータセットを確認してください。 わかりやすくするために、時間ベースのパーティションには partition by 句を使用していません。また、静的フォルダーも使用していません。 次のデータセットは、コピーするデータ (SearchLog tsv) が azure storage の rawdatasample データ/フォルダーにあることを指定します。</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;RawBlobDemoTable&quot;,\n    &quot;properties&quot;: {\n        &quot;published&quot;: false,\n        &quot;type&quot;: &quot;AzureBlob&quot;,\n        &quot;linkedServiceName&quot;: &quot;StorageLinkedService&quot;,\n        &quot;typeProperties&quot;: {\n            &quot;fileName&quot;: &quot;SearchLog.tsv&quot;,\n            &quot;folderPath&quot;: &quot;rawdatasample/data/&quot;,\n            &quot;format&quot;: {\n                &quot;type&quot;: &quot;TextFormat&quot;,\n                &quot;rowDelimiter&quot;: &quot;\\n&quot;,\n                &quot;columnDelimiter&quot;: &quot;\\t&quot;\n            }\n        },\n        &quot;availability&quot;: {\n            &quot;frequency&quot;: &quot;Day&quot;,\n            &quot;interval&quot;: 1,\n            &quot;style&quot;: &quot;StartOfInterval&quot;\n        },\n        &quot;external&quot;: true,\n        &quot;policy&quot;: {\n            &quot;validation&quot;: {\n                &quot;minimumSizeMB&quot;: 0.00001\n            }\n        }\n    }\n}</pre>\n\n<p>Azure Data Lake Store 変換先データセットの作成:</p>\n\n<p>[新しいデータセット-Azure Data Lake Store] を &gt; クリックします。</p>\n\n<p><img alt=\"2015-10-26_08h18_14\" border=\"0\" height=\"439\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/d2c509e0-440c-42a5-a3fa-b8993e9da751.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_08h18_14\" width=\"319\"></p>\n\n<p>これにより、Azure Data Lake Store データセットのテンプレートが表示され、任意の値を入力できます。 例については、以下の Azure Data Lake Store データセットを参照してください。 わかりやすくするために、時間ベースのパーティションには partition by 句を使用していません。また、静的フォルダーも使用していません。 次のデータセットは、data lake の datalake/input/folder にコピーされるデータを指定します。</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;DataLakeTable&quot;,\n    &quot;properties&quot;: {\n        &quot;published&quot;: false,\n        &quot;type&quot;: &quot;AzureDataLakeStore&quot;,\n        &quot;linkedServiceName&quot;: &quot;AzureDataLakeStoreLinkedService&quot;,\n        &quot;typeProperties&quot;: {\n            &quot;folderPath&quot;: &quot;datalake/input/&quot;,\n            &quot;fileName&quot;: &quot;SearchLog.tsv&quot;,\n            &quot;format&quot;: {\n                &quot;type&quot;: &quot;TextFormat&quot;,\n                &quot;rowDelimiter&quot;: &quot;\\n&quot;,\n                &quot;columnDelimiter&quot;: &quot;\\t&quot;\n            }\n        },\n        &quot;availability&quot;: {\n            &quot;frequency&quot;: &quot;Day&quot;,\n            &quot;interval&quot;: 1\n        }\n    }\n}</pre>\n\n<h3>ADF Pipelines の作成</h3>\n\n<p>ADF コピーパイプラインの作成: このパイプラインは、Azure Blob Storage から Azure Data Lake にデータをコピーします。</p>\n\n<p>[新しいパイプライン] をクリックすると、サンプルパイプラインテンプレートが開きます。 たとえば、次のパイプラインでは、Azure blob storage から Azure Data Lake (上記で作成したサンプルデータセット) にデータをコピーします。</p>\n\n<p><u>パイプラインの定義</u>:</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;EgressBlobToDataLakePipeline&quot;,\n    &quot;properties&quot;: {\n        &quot;description&quot;: &quot;Egress data from blob to azure data lake&quot;,\n        &quot;activities&quot;: [\n            {\n                &quot;type&quot;: &quot;Copy&quot;,\n                &quot;typeProperties&quot;: {\n                    &quot;source&quot;: {\n                        &quot;type&quot;: &quot;BlobSource&quot;,\n                        &quot;treatEmptyAsNull&quot;: true\n                    },\n                    &quot;sink&quot;: {\n                        &quot;type&quot;: &quot;AzureDataLakeStoreSink&quot;,\n                        &quot;writeBatchSize&quot;: 0,\n                        &quot;writeBatchTimeout&quot;: &quot;00:00:00&quot;\n                    }\n                },\n                &quot;inputs&quot;: [\n                    {\n                        &quot;name&quot;: &quot;RawBlobDemoTable&quot;\n                    }\n                ],\n                &quot;outputs&quot;: [\n                    {\n                        &quot;name&quot;: &quot;DataLakeTable&quot;\n                    }\n                ],\n                &quot;policy&quot;: {\n                    &quot;timeout&quot;: &quot;10:00:00&quot;,\n                    &quot;concurrency&quot;: 1,\n                    &quot;executionPriorityOrder&quot;: &quot;NewestFirst&quot;,\n                    &quot;retry&quot;: 1\n                },\n                &quot;scheduler&quot;: {\n                    &quot;frequency&quot;: &quot;Day&quot;,\n                    &quot;interval&quot;: 1\n                },\n                &quot;name&quot;: &quot;EgressDataLake&quot;,\n                &quot;description&quot;: &quot;Move data from blob to azure data lake&quot;\n            }\n        ],\n        &quot;start&quot;: &quot;2015-08-08T00:00:00Z&quot;,\n        &quot;end&quot;: &quot;2015-08-08T01:00:00Z&quot;,\n        &quot;isPaused&quot;: false\n    }\n}</pre>\n\n<h3>ADF Pipelines の監視</h3>\n\n<p>上記で作成した ADF コピーパイプラインは、データセットの頻度が1日に設定されていて、パイプライン定義の先頭が08/08/2015 に設定されているため、実行を開始します。 そのため、パイプラインはその日にのみ実行され、コピー操作が1回行われます。 ADF パイプラインのスケジュール設定の詳細について <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-factory-scheduling-and-execution/\">は、ここ</a> をクリックしてください。</p>\n\n<p>ADF ダイアグラムビューに移動して、データファクトリの動作系統を表示します。 Blob Storage から Azure data Lake Store にデータを移動するためのパイプラインと共に、Azure Blob Storage と Azure Data Lake Store データセットが表示されます。</p>\n\n<p><img alt=\"2015-10-27_15h04_32\" border=\"0\" height=\"355\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/e81c7887-4b4b-4b7b-834d-555cd85bf012.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_15h04_32\" width=\"763\"></p>\n\n<p>ダイアグラムビューの DataLakeTable をクリックすると、対応するアクティビティの実行とその状態が表示されます。</p>\n\n<p><img alt=\"2015-10-27_14h51_37\" border=\"0\" height=\"449\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/6b68c345-8a97-4904-8a7e-0accf804243a.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_14h51_37\" width=\"586\"></p>\n\n<p>ADF の EgressBlobToDataLakePipeline のコピーアクティビティ (上記のスクリーンショットを参照) が正常に実行され、3.08 KB のデータが Azure Blob Storage から Azure Data Lake Store にコピーされていることがわかります。 Microsoft Azure ポータルにログインし、Azure Data Lake データエクスプローラーを使用して Azure Data Lake Store にコピーされたデータを視覚化することもできます。</p>\n\n<p><img alt=\"2015-10-27_14h58_37\" border=\"0\" height=\"264\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/4057daff-7511-4076-a66e-75b4be12f232.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_14h58_37\" width=\"729\">&nbsp;</p>\n\n<p>データ移動アクティビティの Azure Data Factory の詳細について <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-factory-data-movement-activities/\">は、ここ</a> をクリックしてください。 ADF での AzureDataLakeStore コネクタの使用に関する詳細なドキュメントについては、 <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-factory-azure-datalake-connector/\">こちら</a>を参照してください。</p>\n\n<h2><font size=\"5\">Azure Data Lake Analytics サービスの処理手順として U-SQL スクリプトを実行する E2E ビッグデータ ADF パイプラインを作成する</font></h2>\n\n<p>複数の業界業界 (小売、財務、ゲーム) の非常に一般的なユースケースは、ログ処理です。</p>\n\n<p>注: 次の手順に従う前に、有効な Azure Data Lake Analytics アカウントを持っている必要があります。 お持ち <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-lake-analytics-get-started-portal/\">で</a> ない場合 &rsquo; は、ここをクリックして新しいアカウントを作成します。</p>\n\n<p>このシナリオでは、前の手順で Azure Data Lake Store アカウントにコピーされたログを使用し、処理手順の1つとして Azure Data Lake Analytics で U-SQL スクリプトを実行してログを処理する ADF パイプラインを作成します。 U-SQL スクリプトは、下流プロセスで使用できるリージョンごとにイベントを計算します。</p>\n\n<p>上記のシナリオで作成したデータ ファクトリ (AzureDataLakeStoreAnalyticsSample) を再利用して、Azure Blob Storage から Azure Data Lake Store にデータをコピーします。</p>\n\n<h3>ADF のリンクされたサービスを作成する</h3>\n\n<p>リンクされたAzure Data Lake Analyticsを作成します。 これは、ログ処理Azure Data Lake Analyticsスクリプトを実行するU-SQLアカウントです。</p>\n\n<p>[新しいコンピューティング] Azure Data Lake Analytics &ndash;&gt; 。</p>\n\n<p><img alt=\"2015-10-26_14h55_59\" border=\"0\" height=\"307\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/e2f7acc0-101e-4feb-9664-4b46a06d15ce.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_14h55_59\" width=\"315\">&nbsp;</p>\n\n<p>リンクされたサービスの必須パラメーター Azure Data Lake Analytics入力します</p>\n\n<ul>\n <li>AccountName: 上記の手順で作成するか、既存のアカウント名を使用して作成します</li>\n <li>承認: このパラメーターを入力するには、[承認] をクリックします。 これによりポップアップが開き、&#39;入力する必要があります。</li>\n</ul>\n\n<p><img alt=\"2015-10-26_15h01_38\" border=\"0\" height=\"323\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/bc515b52-e29d-4054-9f49-cdcd0c7cda3c.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_15h01_38\" width=\"430\">&nbsp;</p>\n\n<p>アカウントが別のサブスクリプション内Azure Data Lake Analytics別のリソース グループ名の下にある場合は、省略可能なパラメーターを入力します。</p>\n\n<ul>\n <li>SubscriptionID</li>\n <li>ResourceGroupName</li>\n</ul>\n\n<p>[デプロイ] をクリックします。 これにより、リンクされたサービスAzure Data Lake Analytics作成されます。</p>\n\n<p>注: Deploy を押す前に値を指定しない場合は、JSON で Optional という行を削除する必要があります。</p>\n\n<p>Azure Data Lake Store のリンクされたサービスの作成: これは、データを移動する Azure Data Lake Storage (シンク別名宛先) です。</p>\n\n<p>注: 上記のコピー シナリオに続いてこのシナリオを実行している場合は、このリンクされたサービスを既に作成している必要があります。</p>\n\n<p>[新しいデータ ストア -&gt; Azure Data Lake Store] をクリックします。</p>\n\n<p><img alt=\"2015-10-25_23h11_00\" border=\"0\" height=\"439\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/985e7079-5f07-478e-b596-03e9e95b6b7f.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_23h11_00\" width=\"319\"></p>\n\n<p>Azure Data Lake Store Linked Service の必須パラメーターを入力します</p>\n\n<p>DataLakeUri: 上記の手順で作成するか、既存の方法を使用して作成)、例: <a href=\"https://&lt;adlstoreaccountname&gt;.azuredatalakestore.net/webhdfs/v1\">https://&lt; adlstoreaccountname.azuredatalakestore.net/webhdfs/v1&gt;</a>。 &lt;adlstoreaccountname を&gt; ADL ストア アカウント名に置き換える。</p>\n\n<p>承認: このパラメーターを入力するには、[承認] をクリックします。 これによりポップアップが開き、資格情報を入力する必要があります。</p>\n\n<p><img alt=\"2015-10-25_23h26_03\" border=\"0\" height=\"387\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/c8fb55ad-7d83-46ed-b4e7-077015a70275.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_23h26_03\" width=\"380\"></p>\n\n<p>Azure Data Lake Store アカウントが別のサブスクリプション内で、データ ファクトリとは異なるリソース グループ名の下にある場合は、次のパラメーターも入力する必要があります。</p>\n\n<ul>\n <li>AccountName</li>\n <li>SubscriptionID</li>\n <li>ResourceGroupName</li>\n</ul>\n\n<p>[デプロイ] をクリックします。 これにより、Azure Data Lake Storeリンクされたサービスが作成されます。</p>\n\n<p>注: Deploy を押す前に値を指定しない場合は、JSON で Optional という行を削除する必要があります。</p>\n\n<h3>ADF DataSet を作成する</h3>\n\n<p>Azure ソース データセットData Lake Store作成します。</p>\n\n<p>注: 上記のコピー シナリオに続いてこのシナリオを実行している場合は、このデータセットを既に作成している必要があります。</p>\n\n<p>[新しいデータセット -&gt; Azure Data Lake Store] をクリックします。</p>\n\n<p><img alt=\"2015-10-26_08h18_14\" border=\"0\" height=\"439\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/cac7b58f-f532-4084-883a-131bc284032b.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_08h18_14\" width=\"319\"></p>\n\n<p>これにより、Azure データセットのテンプレートがData Lake Storeされます。 任意の値を入力できます。</p>\n\n<p>たとえば、次の Azure データセットをData Lake Storeしてください。 わかりやすくするために、時間ベースのパーティションに partitioned by 句を使用し、静的フォルダーを使用しません。 次のデータセットは、Data Lake 内の datalake/input/ フォルダーにコピーされるデータを指定します。</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;DataLakeTable&quot;,\n    &quot;properties&quot;: {\n        &quot;published&quot;: false,\n        &quot;type&quot;: &quot;AzureDataLakeStore&quot;,\n        &quot;linkedServiceName&quot;: &quot;AzureDataLakeStoreLinkedService&quot;,\n        &quot;typeProperties&quot;: {\n            &quot;folderPath&quot;: &quot;datalake/input/&quot;,\n            &quot;fileName&quot;: &quot;SearchLog.tsv&quot;,\n            &quot;format&quot;: {\n                &quot;type&quot;: &quot;TextFormat&quot;,\n                &quot;rowDelimiter&quot;: &quot;\\n&quot;,\n                &quot;columnDelimiter&quot;: &quot;\\t&quot;\n            }\n        },\n        &quot;availability&quot;: {\n            &quot;frequency&quot;: &quot;Day&quot;,\n            &quot;interval&quot;: 1\n        }\n    }\n}</pre>\n\n<p>Azure Data Lake Store&rsquo;データセットを作成します。</p>\n\n<p>[新しいデータセット -&gt; Azure Data Lake Store] をクリックします。</p>\n\n<p>例: 以下の EventsByEnGbRegionTable データセット定義を参照してください。 このデータセットに対応するデータは、AzureDataLakeAnalytics U-SQL &lsquo;スクリプトを実行して、en-gb &ldquo;&rsquo; &lt; ロケールと日付 2012/02/19&rdquo; のすべてのイベントを取得した後に生成されます。</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;EventsByEnGbRegionTable&quot;,\n    &quot;properties&quot;: {\n        &quot;published&quot;: false,\n        &quot;type&quot;: &quot;AzureDataLakeStore&quot;,\n        &quot;linkedServiceName&quot;: &quot;AzureDataLakeStoreLinkedService&quot;,\n        &quot;typeProperties&quot;: {\n            &quot;folderPath&quot;: &quot;datalake/output/&quot;\n        },\n        &quot;availability&quot;: {\n            &quot;frequency&quot;: &quot;Day&quot;,\n            &quot;interval&quot;: 1\n        }\n    }\n}</pre>\n\n<h3>ADF ファイルを作成Pipelines</h3>\n\n<p>ADF AzureDataLakeAnalytics パイプラインを作成する: このパイプラインは、U-SQLアクティビティを実行して処理を実行します。</p>\n\n<p>[新しいパイプライン] をクリックすると、サンプル パイプライン テンプレートが開きます。</p>\n\n<p><img alt=\"2015-10-26_17h24_21\" border=\"0\" height=\"501\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/d99477d6-179b-49a3-97db-c13c9d2f80d7.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_17h24_21\" width=\"663\"></p>\n\n<p>[新しいパイプライン] をクリックした後、[アクティビティの追加] をクリックし、DataLakeAnalyticsU-SQL 追加することもできます。</p>\n\n<p><img alt=\"2015-10-26_17h26_34\" border=\"0\" height=\"500\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/212f31b3-205e-4ad4-96ce-3d9ae0677b90.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_17h26_34\" width=\"598\"></p>\n\n<p>たとえば、次のパイプラインは ADLA U-SQL &lsquo;アクティビティを実行して、en-gb &ldquo;&rsquo; &lt; ロケールと日付 2012/02/19&rdquo; のすべてのイベントを取得します。</p>\n\n<p><u>パイプライン定義</u>:</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;ComputeEventsByEnGbRegionPipeline&quot;,\n    &quot;properties&quot;: {\n        &quot;description&quot;: &quot;This is a pipeline to compute events for en-gb locale and date less than 2012/02/19.&quot;,\n        &quot;activities&quot;: [\n            {\n                &quot;type&quot;: &quot;DataLakeAnalyticsU-SQL&quot;,\n                &quot;typeProperties&quot;: {\n                    &quot;scriptPath&quot;: &quot;scripts\\\\kona\\\\SearchLogProcessing.txt&quot;,\n                    &quot;scriptLinkedService&quot;: &quot;StorageLinkedService&quot;,\n                    &quot;degreeOfParallelism&quot;: 3,\n                    &quot;priority&quot;: 100,\n                    &quot;parameters&quot;: {\n                        &quot;in&quot;: &quot;/datalake/input/SearchLog.tsv&quot;,\n                        &quot;out&quot;: &quot;/datalake/output/Result.tsv&quot;\n                    }\n                },\n                &quot;inputs&quot;: [\n                    {\n                        &quot;name&quot;: &quot;DataLakeTable&quot;\n                    }\n                ],\n                &quot;outputs&quot;: [\n                    {\n                        &quot;name&quot;: &quot;EventsByEnGbRegionTable&quot;\n                    }\n                ],\n                &quot;policy&quot;: {\n                    &quot;timeout&quot;: &quot;06:00:00&quot;,\n                    &quot;concurrency&quot;: 1,\n                    &quot;executionPriorityOrder&quot;: &quot;NewestFirst&quot;,\n                    &quot;retry&quot;: 1\n                },\n                &quot;scheduler&quot;: {\n                    &quot;frequency&quot;: &quot;Day&quot;,\n                    &quot;interval&quot;: 1\n                },\n                &quot;name&quot;: &quot;EventsByRegion&quot;,\n                &quot;linkedServiceName&quot;: &quot;AzureDataLakeAnalyticsLinkedService&quot;\n            }\n        ],\n        &quot;start&quot;: &quot;2015-08-08T00:00:00Z&quot;,\n        &quot;end&quot;: &quot;2015-08-08T01:00:00Z&quot;,\n        &quot;isPaused&quot;: false\n    }\n}</pre>\n\n<p>上記U-SQLによって&lsquo;実行されているスクリプトは、デプロイされた StorageLinkedService に対応する Azure Blob Storage アカウントの scripts/Storage&rsquo; にあります。</p>\n\n<p><u>SearchLogProcessing.txtスクリプト定義:</u></p>\n\n<pre class=\"prettyprint\">\n@searchlog =\n    EXTRACT UserId          int,\n            Start           DateTime,\n            Region          string,\n            Query           string,\n            Duration        int?,\n            Urls            string,\n            ClickedUrls     string\n    FROM @in\n    USING Extractors.Tsv(nullEscape:&quot;#NULL#&quot;);\n\n@rs1 =\n    SELECT Start, Region, Duration\n    FROM @searchlog\nWHERE Region == &quot;en-gb&quot;;\n\n@rs1 =\n    SELECT Start, Region, Duration\n    FROM @rs1\n    WHERE Start &lt;= DateTime.Parse(&quot;2012/02/19&quot;);\n\nOUTPUT @rs1   \n    TO @out\n      USING Outputters.Tsv(quoting:false, dateTimeFormat:null);\n</pre>\n\n<p>上記のスクリプトの @in および @out パラメーターの値はU-SQLパラメーター セクションを使用して ADF によって動的に渡されます。 パイプライン定義の上記の「パラメーター」セクションを参照してください。</p>\n\n<p>他のプロパティ viz(degreeOfParallelism、優先度など) と、Azure Data Lake Analytics サービスで実行されるジョブのパイプライン定義を指定できます。</p>\n\n<h3>ADF の監視Pipelines</h3>\n\n<p>上記の ADF コピー パイプラインは、データセットの頻度が 1 日に設定され、パイプライン定義の開始が 2015 年 8 月 8 日に設定され、実行が開始されます。 そのため、パイプラインは、その日だけ実行され、1 回だけU-SQL実行されます。 ADF <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-factory-scheduling-and-execution/\">パイプライン</a> のスケジュール設定の詳細については、こちらをクリックしてください。</p>\n\n<p>ADF ダイアグラム ビューに移動して、データ ファクトリの運用ラインを表示します。 2 つのパイプラインと、対応するデータセット viz が表示されます。EgressBlobToDataLakePipeline (Azure Blob Storage から Azure Data Lake Store にデータをコピー) と ComputeEventsByEnGbRegionPipeline (&lsquo;en-gb &ldquo;&rsquo; &lt; ロケールと日付 2012/02/19&rdquo; のすべてのイベントを取得します)。</p>\n\n<p><img alt=\"2015-10-27_14h50_04\" border=\"0\" height=\"525\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/611bb838-06be-4ab9-9286-35769f5faab6.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_14h50_04\" width=\"1250\"></p>\n\n<p>ダイアグラム ビューで EventsByEnGbRegionTable をクリックすると、対応するアクティビティの実行とその状態が表示されます。</p>\n\n<p><img alt=\"2015-10-27_15h01_09\" border=\"0\" height=\"614\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/ff7293a5-7d2e-4342-a390-5bec17e16ec9.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_15h01_09\" width=\"583\"></p>\n\n<p>ADF の ComputeEventsByEnGbRegionPipeline の U-SQL アクティビティが正常に実行され、AzureDataLakeStore アカウントに Results.tsv ファイル (/datalake/output/Result.tsv) が作成されているのを確認できます。 Result.tsv には、&lsquo;en-gb &ldquo;&rsquo; &lt; ロケールと日付 2012/02/19&rdquo; のすべてのイベントが含まれている。 Microsoft Azure ポータルにログインし、Azure Data Lake データ エクスプローラー を使用して、Azure Data Lake Store で上記の処理手順の一環として生成された Result.tsv ファイルを視覚化できます。</p>\n\n<p><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/f1fdee9c-20e2-461e-9d47-dc9a0fc259c8.png\"><img alt=\"2015-10-27_15h02_42\" border=\"0\" height=\"272\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/0531d6b8-5a56-409b-97a6-2b529cea2926.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_15h02_42\" width=\"754\"></a>&nbsp;</p>\n\n<p>AzureDataLakeAnalyticsU-SQL&nbsp;の詳細なドキュメントについては、Azure Data Factory<a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-factory-usql-activity/\">覧ください</a>。</p>\n\n<p>まとめると、上記の手順に従って、azure Data Lake Store にデータを移動できる Azure Data Factory を使用して E2E ビッグ データ パイプラインを構築しました。 さらに、処理ステップの 1 U-SQL として Azure Data Lake Analytics スクリプトを実行し、ニーズに応じて動的にスケーリングすることもできます。</p>\n\n<p>ビッグ データの処理と分析のワークフローを運用可能にするソリューションに引き続き投資します。 <a href=\"https://azure.microsoft.com/en-us/solutions/data-lake/\">Microsoft Cloud</a> Platform チームの Data Lake Microsoft Azure詳細については、こちらをクリックしてください。 データ ファクトリを使用して簡単かつ迅速Azure Data Factoryパイプラインを構築し<a href=\"https://azure.microsoft.com/en-us/documentation/services/data-factory/\"></a>、データ ファクトリを試す場合は、こちらを参照してください。 機能に関するリクエストがある場合、またはデータ ファクトリに関するフィードバックを提供したい場合は、Azure Data Factory <a href=\"https://feedback.azure.com/forums/270578-azure-data-factory\">してください</a>。</p>"
